{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f16f8ea-d198-42e5-b0fe-09608bbcc7c7",
   "metadata": {},
   "source": [
    "# Caching the dataframe(s) in case of a pipeline failure\n",
    "\n",
    "This notebook shows a couple of useful features to help the user to debug a broken pipeline.\n",
    "\n",
    "Please note that this debugging process is intended for execution in a notebook, as it relies on nebula storage, which resides within the Python kernel and cannot be utilized in a recipe or Airflow.\n",
    "\n",
    "There two main situations in which a pipeline may break:\n",
    "- a transformer fails\n",
    "- In a split pipeline, the split dataframes become unmergeable due to varying schemas or columns.\n",
    "\n",
    "In the first case nebula stores the input dataframe of the failed transformer, in the latter one all the dataframes that should be merged are retained, allowing the user to retrieve them and address the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fc11c7-5f40-4a82-98f1-a8e7b0cf4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from nebula import nebula_storage as ns\n",
    "from nebula import TransformerPipeline\n",
    "from nebula.base import Transformer\n",
    "from nebula.transformers import (\n",
    "    AssertNotEmpty,\n",
    "    DropColumns,\n",
    "    RenameColumns,\n",
    "    SelectColumns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d10e422-f6be-445b-a5c7-5a545592e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ThisTransformerIsBroken(Transformer):\n",
    "    @staticmethod\n",
    "    def _transform_nw(df):\n",
    "        raise ValueError(\"Broken transformer\")\n",
    "\n",
    "\n",
    "df_input = pl.DataFrame({\n",
    "    \"c1\": [1, 2],\n",
    "    \"c2\": [3, 4],\n",
    "    \"c3\": [5, 6],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d62f015-84f1-460c-9d4f-2cf933beef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 22:29:32,424 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-25 22:29:32,428 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-25 22:29:32,429 | [INFO]: Nebula Storage: setting an object (<class 'int'>) with the key \"AASASDS\". \n",
      "2025-12-25 22:29:32,429 | [INFO]: Nebula Storage: setting an object (<class 'int'>) with the key \"AASAsdSDS\". \n",
      "2025-12-25 22:29:32,429 | [INFO]: Starting pipeline \n",
      "2025-12-25 22:29:32,429 | [INFO]: Running 'ThisTransformerIsBroken' ... \n",
      "2025-12-25 22:29:32,439 | [ERROR]: Error at node ThisTransformerIsBroken_b01eb6@1: Broken transformer \n",
      "2025-12-25 22:29:32,439 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"FAIL_DF_transformer:ThisTransformerIsBroken\". \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Get the dataframe(s) before the failure in the nebula storage with the keys: ['transformer:ThisTransformerIsBroken']\nOriginal Error:\nBroken transformer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:121\u001b[0m, in \u001b[0;36mPipelineExecutor.run\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# Execute the IR tree\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Handle failure - store cached DFs if any\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:154\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_node\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, SequenceNode):\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, TransformerNode):\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:175\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_sequence\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39msteps:\n\u001b[1;32m--> 175\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ctx\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:156\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_node\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, TransformerNode):\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, FunctionNode):\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:187\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_transformer\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# Run forced interleaved if set\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\base.py:92\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     91\u001b[0m df_nw \u001b[38;5;241m=\u001b[39m nw\u001b[38;5;241m.\u001b[39mfrom_native(df)\n\u001b[1;32m---> 92\u001b[0m df_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_nw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_nw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nw\u001b[38;5;241m.\u001b[39mto_native(df_out)\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mThisTransformerIsBroken._transform_nw\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_transform_nw\u001b[39m(df):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroken transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Broken transformer",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# _FAIL_CACHE.clear()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m TransformerPipeline([ThisTransformerIsBroken(), AssertNotEmpty()])\n\u001b[1;32m----> 8\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\pipeline.py:503\u001b[0m, in \u001b[0;36mTransformerPipeline.run\u001b[1;34m(self, df, hooks, resume_from, show_params, force_interleaved_transformer)\u001b[0m\n\u001b[0;32m    493\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m LoggingHooks(max_param_length\u001b[38;5;241m=\u001b[39mPIPE_CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_param_length\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    494\u001b[0m                          show_params\u001b[38;5;241m=\u001b[39mshow_params)\n\u001b[0;32m    496\u001b[0m executor \u001b[38;5;241m=\u001b[39m PipelineExecutor(\n\u001b[0;32m    497\u001b[0m     ir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ir,\n\u001b[0;32m    498\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    499\u001b[0m     resume_from\u001b[38;5;241m=\u001b[39mresume_from,\n\u001b[0;32m    500\u001b[0m     force_interleaved\u001b[38;5;241m=\u001b[39mforce_interleaved_transformer,\n\u001b[0;32m    501\u001b[0m )\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:130\u001b[0m, in \u001b[0;36mPipelineExecutor.run\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m    127\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet the dataframe(s) before the failure in the nebula \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage with the keys: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal Error:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_fail_cache(ctx)\n\u001b[1;32m--> 130\u001b[0m         \u001b[43mraise_pipeline_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Notify hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\exceptions.py:66\u001b[0m, in \u001b[0;36mraise_pipeline_error\u001b[1;34m(e, msg)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Fallback: wrap in same exception type with enhanced message\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Get the dataframe(s) before the failure in the nebula storage with the keys: ['transformer:ThisTransformerIsBroken']\nOriginal Error:\nBroken transformer"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "ns.set(\"AASASDS\", 123)\n",
    "ns.set(\"AASAsdSDS\", 12653)\n",
    "# _FAIL_CACHE.clear()\n",
    "\n",
    "pipe = TransformerPipeline([ThisTransformerIsBroken(), AssertNotEmpty()])\n",
    "\n",
    "pipe.run(df_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5a1a4-6e95-44aa-99c0-dc9326ed0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.list_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab3a9fe-f7e2-411e-9dd1-fa83a68a95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# name = _PREFIX_FAIL_CACHE + \"ThisTransformerIsBroken\"\n",
    "# print(ns.list_keys())\n",
    "\n",
    "# df_chk = ns.get(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5871b59e-1fc4-418b-b0a2-ce47e55102c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 22:29:34,843 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-25 22:29:34,844 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-25 22:29:34,844 | [INFO]: Starting pipeline \n",
      "2025-12-25 22:29:34,845 | [INFO]: Entering split \n",
      "2025-12-25 22:29:34,845 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"FAIL_DF_fork:split\". \n"
     ]
    },
    {
     "ename": "ColumnNotFoundError",
     "evalue": "Get the dataframe(s) before the failure in the nebula storage with the keys: ['fork:split']\nOriginal Error:\nunable to find column \"cad1\"; valid columns: [\"c1\", \"c2\", \"c3\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mColumnNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:121\u001b[0m, in \u001b[0;36mPipelineExecutor.run\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# Execute the IR tree\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Handle failure - store cached DFs if any\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:154\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_node\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, SequenceNode):\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, TransformerNode):\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:175\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_sequence\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39msteps:\n\u001b[1;32m--> 175\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ctx\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:162\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_node\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, ForkNode):\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, MergeNode):\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:268\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_fork\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mfork_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 268\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_split_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mfork_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbranch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:285\u001b[0m, in \u001b[0;36mPipelineExecutor._execute_split_fork\u001b[1;34m(self, node, ctx)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Call split function\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m split_dfs \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Verify keys match\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36m_split_function\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      4\u001b[0m cond \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcad1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m: df\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;241m~\u001b[39mcond),\n\u001b[0;32m      8\u001b[0m }\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\venv\\lib\\site-packages\\polars\\dataframe\\frame.py:5325\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[1;34m(self, *predicates, **constraints)\u001b[0m\n\u001b[0;32m   5322\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpolars\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazyframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[0;32m   5324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 5325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5326\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredicates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQueryOptFlags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5328\u001b[0m )\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\venv\\lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[0m, in \u001b[0;36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\venv\\lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[0m, in \u001b[0;36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m optflags\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\venv\\lib\\site-packages\\polars\\lazyframe\\frame.py:2429\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[0m\n\u001b[0;32m   2428\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[1;32m-> 2429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mColumnNotFoundError\u001b[0m: unable to find column \"cad1\"; valid columns: [\"c1\", \"c2\", \"c3\"]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mColumnNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m: [DropColumns(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc2\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m: [DropColumns(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc3\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     14\u001b[0m pipe \u001b[38;5;241m=\u001b[39m TransformerPipeline(data, split_function\u001b[38;5;241m=\u001b[39m_split_function)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\pipeline.py:503\u001b[0m, in \u001b[0;36mTransformerPipeline.run\u001b[1;34m(self, df, hooks, resume_from, show_params, force_interleaved_transformer)\u001b[0m\n\u001b[0;32m    493\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m LoggingHooks(max_param_length\u001b[38;5;241m=\u001b[39mPIPE_CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_param_length\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    494\u001b[0m                          show_params\u001b[38;5;241m=\u001b[39mshow_params)\n\u001b[0;32m    496\u001b[0m executor \u001b[38;5;241m=\u001b[39m PipelineExecutor(\n\u001b[0;32m    497\u001b[0m     ir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ir,\n\u001b[0;32m    498\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    499\u001b[0m     resume_from\u001b[38;5;241m=\u001b[39mresume_from,\n\u001b[0;32m    500\u001b[0m     force_interleaved\u001b[38;5;241m=\u001b[39mforce_interleaved_transformer,\n\u001b[0;32m    501\u001b[0m )\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\execution\\executor.py:130\u001b[0m, in \u001b[0;36mPipelineExecutor.run\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m    127\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet the dataframe(s) before the failure in the nebula \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage with the keys: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal Error:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_fail_cache(ctx)\n\u001b[1;32m--> 130\u001b[0m         \u001b[43mraise_pipeline_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Notify hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\pypr\\gitrepos\\nebula\\src\\nebula\\pipelines\\exceptions.py:66\u001b[0m, in \u001b[0;36mraise_pipeline_error\u001b[1;34m(e, msg)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Fallback: wrap in same exception type with enhanced message\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mColumnNotFoundError\u001b[0m: Get the dataframe(s) before the failure in the nebula storage with the keys: ['fork:split']\nOriginal Error:\nunable to find column \"cad1\"; valid columns: [\"c1\", \"c2\", \"c3\"]"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "def _split_function(df):\n",
    "    cond = pl.col(\"cad1\") < 2\n",
    "    return {\n",
    "        \"low\": df.filter(cond),\n",
    "        \"hi\": df.filter(~cond),\n",
    "    }\n",
    "    \n",
    "data = {\n",
    "    \"low\": [DropColumns(columns=\"c2\")],\n",
    "    \"hi\": [DropColumns(columns=\"c3\")],\n",
    "}\n",
    "pipe = TransformerPipeline(data, split_function=_split_function)\n",
    "\n",
    "pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7da971-ef38-474d-8f5a-595166738d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.list_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00a0d3-4ef3-49d3-993f-be675cb623d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce30c84-88bd-4739-83ec-6d13e83acebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [4.1234, \"\", \"\"],\n",
    "    [5.1234, None, None],\n",
    "    [6.1234, \"\", None],\n",
    "    [8.1234, \"a\", None],\n",
    "    [9.1234, \"a\", \"\"],\n",
    "    [10.1234, \"\", \"b\"],\n",
    "    [11.1234, \"a\", None],\n",
    "    [12.1234, None, \"b\"],\n",
    "    [14.1234, \"\", None],\n",
    "]\n",
    "\n",
    "df_input = pl.DataFrame(data, orient=\"row\", schema=[\"c1\", \"c2\", \"c3\"])\n",
    "print(df_input.schema)\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166b6ba-efda-4433-8eb5-ac933943873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_function_with_null(df: pl.DataFrame) -> dict[str, pl.DataFrame]:\n",
    "    \"\"\"Split dataframe into 'low', 'hi', and 'null' subsets.\"\"\"\n",
    "    ret = _split_function(df)\n",
    "    # Include both actual nulls and NaN values in the 'null' split\n",
    "    cond_null = pl.col(\"c1\").is_null() | pl.col(\"c1\").is_nan()\n",
    "    return {**ret, \"null\": df.filter(cond_null)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0bcef-5561-4338-8994-094a3ff2b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_transformers = {\"low\": [], \"hi\": []}\n",
    "\n",
    "pipe = TransformerPipeline(\n",
    "    dict_transformers,\n",
    "    split_function=_split_function_with_null,\n",
    "    splits_no_merge={\"hi\"},\n",
    ")\n",
    "\n",
    "pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f414e36-652d-4cf6-aec3-e106f1b46ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ee1e4-d59f-4fef-b7c0-8172bd82302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe._ir.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64598d83-b06a-4b09-b99f-a87d0600f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "callable(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da247af-df3a-41aa-879b-0abcfe38bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    StructField(\"c1\", FloatType(), True),\n",
    "    StructField(\"c2\", StringType(), True),\n",
    "    StructField(\"c3\", StringType(), True),\n",
    "]\n",
    "\n",
    "data = [\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [1.1234, \"a\", \"  b\"],\n",
    "    [2.1234, \"  a  \", \"  b  \"],\n",
    "    [3.1234, \"\", \"\"],\n",
    "    [4.1234, \"   \", \"   \"],\n",
    "    [5.1234, None, None],\n",
    "    [6.1234, \" \", None],\n",
    "    [7.1234, \"\", None],\n",
    "    [8.1234, \"a\", None],\n",
    "    [9.1234, \"a\", \"\"],\n",
    "    [10.1234, \"   \", \"b\"],\n",
    "    [11.1234, \"a\", None],\n",
    "    [12.1234, None, \"b\"],\n",
    "    [13.1234, None, \"b\"],\n",
    "    [14.1234, None, None],\n",
    "]\n",
    "\n",
    "df_input = spark.createDataFrame(data, schema=StructType(schema)).cache()\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f913e-e8f7-4891-88b4-569fda9e946a",
   "metadata": {},
   "source": [
    "## Transformer failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d109f1-056a-4e4d-8c1a-1b1b84eebe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThisTransformerIsBroken:\n",
    "    @staticmethod\n",
    "    def transform(df):\n",
    "        \"\"\"Public transform method w/o parent class.\"\"\"        \n",
    "        return df.select(\"wrong\")\n",
    "\n",
    "\n",
    "# clear the cache\n",
    "ns.clear()\n",
    "\n",
    "pipe = TransformerPipeline([\n",
    "    NanToNull(input_columns=\"*\"),\n",
    "    ThisTransformerIsBroken(),\n",
    "    Distinct(),\n",
    "])\n",
    "\n",
    "pipe.show_pipeline(add_transformer_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016febd-a9b7-482e-bb0a-3cca7c6585aa",
   "metadata": {},
   "source": [
    "### Retrieve the input dataframe of the failed transformer as the pipe breaks.\n",
    "\n",
    "The error message will contain the key(s) associated with storing the aforementioned dataframe. \n",
    "\n",
    "A few lines above, the original exception is documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b96c7-db3f-4490-98bd-d820cf017fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d0a64-b046-4d7f-89c9-71fccafa10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.get(\"FAIL_DF_ThisTransformerIsBroken\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ba961-635e-4674-86b7-8c8c67d3ff3a",
   "metadata": {},
   "source": [
    "## Unable to merge splits\n",
    "\n",
    "In this example the transformers work properly, but they modified the dataframes in a way that is not possible to merge them back anymore.\n",
    "\n",
    "To address this issue, all the dataframes before the union process are stored, allowing the user to investigate the problem.\n",
    "\n",
    "In this example one split drops the column `c2`, the other one the column `c3`, hence they cannot be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff404d-0438-4d90-bd11-7829e38c0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split_function(df):\n",
    "    cond = F.col(\"c1\") < 10\n",
    "    return {\n",
    "        \"low\": df.filter(cond),\n",
    "        \"hi\": df.filter(~cond),\n",
    "    }\n",
    "\n",
    "\n",
    "dict_transf = {\n",
    "    \"low\": [DropColumns(columns=\"c2\")],\n",
    "    \"hi\": [DropColumns(columns=\"c3\")],\n",
    "}\n",
    "\n",
    "# clear the cache\n",
    "ns.clear()\n",
    "\n",
    "pipe = TransformerPipeline(dict_transf, split_function=my_split_function)\n",
    "\n",
    "pipe.show_pipeline(add_transformer_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576726f-8d54-4cc4-8625-7619bb6efda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789f772-d861-465a-b046-fcfb4c1b249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.get(\"FAIL_DF_low\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282a45e-046c-4c5e-9faa-c33e9016e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.get(\"FAIL_DF_hi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefc67f-8f77-4d85-8fe4-7e879ee0ce19",
   "metadata": {},
   "source": [
    "### Overwriting keys associated with the failed dataframes\n",
    "\n",
    "The keys used for storing dataframes are generated with a method that prevent any form of overwriting by adding a numerical suffix to them, hence the user should not worry about that.\n",
    "\n",
    "Rerruning the same broken pipeline, without clearing the cache, the keys associated with the failed dataframes do not overwrite the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e8968-792a-48ab-8efb-fb08a610d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b785e5-6958-4583-a0f0-8c0eef371960",
   "metadata": {},
   "source": [
    "The keys associated with the failed dataframes are now:\n",
    "- `FAIL_DF_low_0`\n",
    "- `FAIL_DF_hi_0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849db7d-126d-4f9f-a6e9-bd76d7053c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
