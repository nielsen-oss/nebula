{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f16f8ea-d198-42e5-b0fe-09608bbcc7c7",
   "metadata": {},
   "source": [
    "# Caching the dataframe(s) in case of a pipeline failure\n",
    "\n",
    "This notebook shows a couple of useful features to help the user to debug a broken pipeline.\n",
    "\n",
    "Please note that this debugging process is intended for execution in a notebook, as it relies on nebula storage, which resides within the Python kernel and cannot be utilized in a recipe or Airflow.\n",
    "\n",
    "There two main situations in which a pipeline may break:\n",
    "- a transformer fails\n",
    "- In a split pipeline, the split dataframes become unmergeable due to varying schemas or columns.\n",
    "\n",
    "In the first case nebula stores the input dataframe of the failed transformer, in the latter one all the dataframes that should be merged are retained, allowing the user to retrieve them and address the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fc11c7-5f40-4a82-98f1-a8e7b0cf4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: 3.9\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import yaml\n",
    "\n",
    "from nlsn.nebula.spark_transformers import *\n",
    "from nlsn.nebula.base import Transformer\n",
    "from nlsn.nebula.pipelines.pipelines import TransformerPipeline\n",
    "from nlsn.nebula.pipelines.pipeline_loader import load_pipeline\n",
    "from nlsn.nebula.storage import nebula_storage as ns\n",
    "\n",
    "py_version = \".\".join(map(str, (sys.version_info[0:2])))\n",
    "print(\"python version:\", py_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bcb96cc-5907-4bd9-9e2e-1b04df3dc520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/16 14:05:08 WARN [Thread-3] Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da247af-df3a-41aa-879b-0abcfe38bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|     c1|   c2|   c3|\n",
      "+-------+-----+-----+\n",
      "| 0.1234|    a|    b|\n",
      "| 0.1234|    a|    b|\n",
      "| 0.1234|    a|    b|\n",
      "| 1.1234|    a|    b|\n",
      "| 2.1234|  a  |  b  |\n",
      "| 3.1234|     |     |\n",
      "| 4.1234|     |     |\n",
      "| 5.1234| null| null|\n",
      "| 6.1234|     | null|\n",
      "| 7.1234|     | null|\n",
      "| 8.1234|    a| null|\n",
      "| 9.1234|    a|     |\n",
      "|10.1234|     |    b|\n",
      "|11.1234|    a| null|\n",
      "|12.1234| null|    b|\n",
      "|13.1234| null|    b|\n",
      "|14.1234| null| null|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = [\n",
    "    StructField(\"c1\", FloatType(), True),\n",
    "    StructField(\"c2\", StringType(), True),\n",
    "    StructField(\"c3\", StringType(), True),\n",
    "]\n",
    "\n",
    "data = [\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [1.1234, \"a\", \"  b\"],\n",
    "    [2.1234, \"  a  \", \"  b  \"],\n",
    "    [3.1234, \"\", \"\"],\n",
    "    [4.1234, \"   \", \"   \"],\n",
    "    [5.1234, None, None],\n",
    "    [6.1234, \" \", None],\n",
    "    [7.1234, \"\", None],\n",
    "    [8.1234, \"a\", None],\n",
    "    [9.1234, \"a\", \"\"],\n",
    "    [10.1234, \"   \", \"b\"],\n",
    "    [11.1234, \"a\", None],\n",
    "    [12.1234, None, \"b\"],\n",
    "    [13.1234, None, \"b\"],\n",
    "    [14.1234, None, None],\n",
    "]\n",
    "\n",
    "df_input = spark.createDataFrame(data, schema=StructType(schema)).cache()\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f913e-e8f7-4891-88b4-569fda9e946a",
   "metadata": {},
   "source": [
    "## Transformer failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d109f1-056a-4e4d-8c1a-1b1b84eebe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:05:16,654 | storage.py:108 [INFO]: Nebula Storage: clear. \n",
      "2024-05-16 14:05:16,655 | storage.py:118 [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TransformerPipeline *** (3 transformers)\n",
      " - NanToNull -> PARAMS: input_columns=\"*\"\n",
      " - ThisTransformerIsBroken\n",
      " - Distinct\n"
     ]
    }
   ],
   "source": [
    "class ThisTransformerIsBroken:\n",
    "    @staticmethod\n",
    "    def transform(df):\n",
    "        \"\"\"Public transform method w/o parent class.\"\"\"        \n",
    "        return df.select(\"wrong\")\n",
    "\n",
    "\n",
    "# clear the cache\n",
    "ns.clear()\n",
    "\n",
    "pipe = TransformerPipeline([\n",
    "    NanToNull(input_columns=\"*\"),\n",
    "    ThisTransformerIsBroken(),\n",
    "    Distinct(),\n",
    "])\n",
    "\n",
    "pipe.show_pipeline(add_transformer_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016febd-a9b7-482e-bb0a-3cca7c6585aa",
   "metadata": {},
   "source": [
    "### Retrieve the input dataframe of the failed transformer as the pipe breaks.\n",
    "\n",
    "The error message will contain the key(s) associated with storing the aforementioned dataframe. \n",
    "\n",
    "A few lines above, the original exception is documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253b96c7-db3f-4490-98bd-d820cf017fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:05:16,765 | pipelines.py:516 [INFO]: Running *** TransformerPipeline *** (3 transformers) \n",
      "2024-05-16 14:05:16,766 | pipelines.py:283 [INFO]: Running NanToNull -> PARAMS: input_columns=\"*\" ... \n",
      "2024-05-16 14:05:16,950 | pipelines.py:297 [INFO]: Execution time for NanToNull: 0.2s \n",
      "2024-05-16 14:05:16,950 | pipelines.py:283 [INFO]: Running ThisTransformerIsBroken ... \n",
      "2024-05-16 14:05:17,079 | storage.py:124 [INFO]: Nebula Storage: setting an object (<class 'pyspark.sql.dataframe.DataFrame'>) with the key \"FAIL_DF_ThisTransformerIsBroken\". \n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Find the input dataframe of the failed transformer \"ThisTransformerIsBroken\" in the nebula storage by using the key \"FAIL_DF_ThisTransformerIsBroken\".\ncannot resolve 'wrong' given input columns: [c1, c2, c3];\n'Project ['wrong]\n+- Project [CASE WHEN isnan(c1#0) THEN cast(null as float) ELSE c1#0 END AS c1#94, c2#1, c3#2]\n   +- LogicalRDD [c1#0, c2#1, c3#2], false\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mpipe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_input\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:1301\u001B[0m, in \u001B[0;36mTransformerPipeline.run\u001B[0;34m(self, df_input)\u001B[0m\n\u001B[1;32m   1298\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m   1299\u001B[0m appending_func \u001B[38;5;241m=\u001B[39m pyspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39munionByName\n\u001B[0;32m-> 1301\u001B[0m df_ret \u001B[38;5;241m=\u001B[39m \u001B[43m_run_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mappending_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m start_time \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1304\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m time()\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:518\u001B[0m, in \u001B[0;36m_run_pipeline\u001B[0;34m(obj, df, appending_func)\u001B[0m\n\u001B[1;32m    516\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunning \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mget_pipeline_name(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    517\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunning the stages: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m__names_in_iterable(obj\u001B[38;5;241m.\u001B[39mstages)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 518\u001B[0m         df \u001B[38;5;241m=\u001B[39m \u001B[43m_run_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mappending_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;66;03m# obj is a split pipeline\u001B[39;00m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mget_pipe_type() \u001B[38;5;241m==\u001B[39m NodeType\u001B[38;5;241m.\u001B[39mSPLIT_PIPELINE:\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:438\u001B[0m, in \u001B[0;36m_run_pipeline\u001B[0;34m(obj, df, appending_func)\u001B[0m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_plain_transformer_list(obj):  \u001B[38;5;66;03m# plain list of transformers\u001B[39;00m\n\u001B[1;32m    437\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput is a plain list/tuple of Transformers: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnames\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 438\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    441\u001B[0m     \u001B[38;5;66;03m# Handle mixed list/tuple of Transformers/Pipelines\u001B[39;00m\n\u001B[1;32m    442\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput is a mixed list of Transformers/Pipelines: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnames\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:294\u001B[0m, in \u001B[0;36m_transform\u001B[0;34m(stages, df)\u001B[0m\n\u001B[1;32m    292\u001B[0m     _, msg, _ \u001B[38;5;241m=\u001B[39m _cache_to_nebula_storage()\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;66;03m# raise PipelineError(msg) from e\u001B[39;00m\n\u001B[0;32m--> 294\u001B[0m     \u001B[43mraise_pipeline_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m t_run: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m time() \u001B[38;5;241m-\u001B[39m t_start\n\u001B[1;32m    297\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecution time for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt_run\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/exceptions.py:45\u001B[0m, in \u001B[0;36mraise_pipeline_error\u001B[0;34m(e, msg)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m HAS_SPARK \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, CapturedException):\n\u001B[1;32m     44\u001B[0m     e\u001B[38;5;241m.\u001B[39mdesc \u001B[38;5;241m=\u001B[39m msg \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m e\u001B[38;5;241m.\u001B[39mdesc\n\u001B[0;32m---> 45\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(e)(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:290\u001B[0m, in \u001B[0;36m_transform\u001B[0;34m(stages, df)\u001B[0m\n\u001B[1;32m    288\u001B[0m t_start: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m    289\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 290\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    292\u001B[0m     _, msg, _ \u001B[38;5;241m=\u001B[39m _cache_to_nebula_storage()\n",
      "Cell \u001B[0;32mIn[4], line 5\u001B[0m, in \u001B[0;36mThisTransformerIsBroken.transform\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(df):\n\u001B[1;32m      4\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Public transform method w/o parent class.\"\"\"\u001B[39;00m        \n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwrong\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/pyspark/sql/dataframe.py:1685\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   1664\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols):\n\u001B[1;32m   1665\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   1666\u001B[0m \n\u001B[1;32m   1667\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1683\u001B[0m \u001B[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001B[39;00m\n\u001B[1;32m   1684\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1685\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1686\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msql_ctx)\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1304\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1305\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1306\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1308\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1309\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1310\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1312\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1313\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/pyspark/sql/utils.py:117\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    113\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Find the input dataframe of the failed transformer \"ThisTransformerIsBroken\" in the nebula storage by using the key \"FAIL_DF_ThisTransformerIsBroken\".\ncannot resolve 'wrong' given input columns: [c1, c2, c3];\n'Project ['wrong]\n+- Project [CASE WHEN isnan(c1#0) THEN cast(null as float) ELSE c1#0 END AS c1#94, c2#1, c3#2]\n   +- LogicalRDD [c1#0, c2#1, c3#2], false\n"
     ]
    }
   ],
   "source": [
    "pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5d0a64-b046-4d7f-89c9-71fccafa10dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|     c1|   c2|   c3|\n",
      "+-------+-----+-----+\n",
      "| 0.1234|    a|    b|\n",
      "| 0.1234|    a|    b|\n",
      "| 0.1234|    a|    b|\n",
      "| 1.1234|    a|    b|\n",
      "| 2.1234|  a  |  b  |\n",
      "| 3.1234|     |     |\n",
      "| 4.1234|     |     |\n",
      "| 5.1234| null| null|\n",
      "| 6.1234|     | null|\n",
      "| 7.1234|     | null|\n",
      "| 8.1234|    a| null|\n",
      "| 9.1234|    a|     |\n",
      "|10.1234|     |    b|\n",
      "|11.1234|    a| null|\n",
      "|12.1234| null|    b|\n",
      "|13.1234| null|    b|\n",
      "|14.1234| null| null|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ns.get(\"FAIL_DF_ThisTransformerIsBroken\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ba961-635e-4674-86b7-8c8c67d3ff3a",
   "metadata": {},
   "source": [
    "## Unable to merge splits\n",
    "\n",
    "In this example the transformers work properly, but they modified the dataframes in a way that is not possible to merge them back anymore.\n",
    "\n",
    "To address this issue, all the dataframes before the union process are stored, allowing the user to investigate the problem.\n",
    "\n",
    "In this example one split drops the column `c2`, the other one the column `c3`, hence they cannot be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ff404d-0438-4d90-bd11-7829e38c0765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:05:25,993 | storage.py:108 [INFO]: Nebula Storage: clear. \n",
      "2024-05-16 14:05:25,994 | storage.py:118 [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TransformerPipeline *** (2 transformers)\n",
      "SPLIT <<< hi >>>:\n",
      "     - DropColumns -> PARAMS: columns=\"c3\"\n",
      "SPLIT <<< low >>>:\n",
      "     - DropColumns -> PARAMS: columns=\"c2\"\n",
      "MERGE SPLITS:\n",
      "   - <<< hi >>>\n",
      "   - <<< low >>>\n"
     ]
    }
   ],
   "source": [
    "def my_split_function(df):\n",
    "    cond = F.col(\"c1\") < 10\n",
    "    return {\n",
    "        \"low\": df.filter(cond),\n",
    "        \"hi\": df.filter(~cond),\n",
    "    }\n",
    "\n",
    "\n",
    "dict_transf = {\n",
    "    \"low\": [DropColumns(columns=\"c2\")],\n",
    "    \"hi\": [DropColumns(columns=\"c3\")],\n",
    "}\n",
    "\n",
    "# clear the cache\n",
    "ns.clear()\n",
    "\n",
    "pipe = TransformerPipeline(dict_transf, split_function=my_split_function)\n",
    "\n",
    "pipe.show_pipeline(add_transformer_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7576726f-8d54-4cc4-8625-7619bb6efda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:05:27,069 | pipelines.py:523 [INFO]: Running *** TransformerPipeline *** (2 transformers) \n",
      "2024-05-16 14:05:27,164 | pipelines.py:556 [INFO]: Running SPLIT <<< hi >>> \n",
      "2024-05-16 14:05:27,165 | pipelines.py:283 [INFO]: Running DropColumns -> PARAMS: columns=\"c3\" ... \n",
      "2024-05-16 14:05:27,249 | pipelines.py:297 [INFO]: Execution time for DropColumns: 0.1s \n",
      "2024-05-16 14:05:27,249 | pipelines.py:556 [INFO]: Running SPLIT <<< low >>> \n",
      "2024-05-16 14:05:27,250 | pipelines.py:283 [INFO]: Running DropColumns -> PARAMS: columns=\"c2\" ... \n",
      "2024-05-16 14:05:27,255 | pipelines.py:297 [INFO]: Execution time for DropColumns: 0.0s \n",
      "2024-05-16 14:05:27,256 | pipelines.py:570 [INFO]: Merging splits: hi, low ... \n",
      "2024-05-16 14:05:27,347 | storage.py:124 [INFO]: Nebula Storage: setting an object (<class 'pyspark.sql.dataframe.DataFrame'>) with the key \"FAIL_DF_hi\". \n",
      "2024-05-16 14:05:27,348 | storage.py:124 [INFO]: Nebula Storage: setting an object (<class 'pyspark.sql.dataframe.DataFrame'>) with the key \"FAIL_DF_low\". \n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Unable to perform 'unionByName' and append the dataframes. Find the output dataframes of each single split \"hi\", \"low\" in the nebula storage by using the keys \"FAIL_DF_hi\", \"FAIL_DF_low\" respectively.\nCannot resolve column name \"c2\" among (c1, c3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mpipe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_input\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:1301\u001B[0m, in \u001B[0;36mTransformerPipeline.run\u001B[0;34m(self, df_input)\u001B[0m\n\u001B[1;32m   1298\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m   1299\u001B[0m appending_func \u001B[38;5;241m=\u001B[39m pyspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39munionByName\n\u001B[0;32m-> 1301\u001B[0m df_ret \u001B[38;5;241m=\u001B[39m \u001B[43m_run_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mappending_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m start_time \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1304\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m time()\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:588\u001B[0m, in \u001B[0;36m_run_pipeline\u001B[0;34m(obj, df, appending_func)\u001B[0m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    587\u001B[0m     _, msg, _ \u001B[38;5;241m=\u001B[39m _cache_to_nebula_storage()\n\u001B[0;32m--> 588\u001B[0m     \u001B[43mraise_pipeline_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mrepartition_output_to_original:\n\u001B[1;32m    591\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mrepartition(n_part)\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/exceptions.py:45\u001B[0m, in \u001B[0;36mraise_pipeline_error\u001B[0;34m(e, msg)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m HAS_SPARK \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, CapturedException):\n\u001B[1;32m     44\u001B[0m     e\u001B[38;5;241m.\u001B[39mdesc \u001B[38;5;241m=\u001B[39m msg \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m e\u001B[38;5;241m.\u001B[39mdesc\n\u001B[0;32m---> 45\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(e)(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:585\u001B[0m, in \u001B[0;36m_run_pipeline\u001B[0;34m(obj, df, appending_func)\u001B[0m\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m obj\u001B[38;5;241m.\u001B[39msplit_cast_to_input_schema:\n\u001B[1;32m    584\u001B[0m         li_df_split \u001B[38;5;241m=\u001B[39m [cast_to_schema(i, input_schema) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m li_df_split]\n\u001B[0;32m--> 585\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mappending_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mli_df_split\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    587\u001B[0m     _, msg, _ \u001B[38;5;241m=\u001B[39m _cache_to_nebula_storage()\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/pyspark/sql/dataframe.py:1901\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n\u001B[1;32m   1858\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other, allowMissingColumns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1859\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   1860\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   1861\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1899\u001B[0m \u001B[38;5;124;03m       missing columns.\u001B[39;00m\n\u001B[1;32m   1900\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1901\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msql_ctx)\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1304\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1305\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1306\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1308\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1309\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1310\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1312\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1313\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/pyspark/sql/utils.py:117\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    113\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Unable to perform 'unionByName' and append the dataframes. Find the output dataframes of each single split \"hi\", \"low\" in the nebula storage by using the keys \"FAIL_DF_hi\", \"FAIL_DF_low\" respectively.\nCannot resolve column name \"c2\" among (c1, c3)"
     ]
    }
   ],
   "source": [
    "_ = pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1789f772-d861-465a-b046-fcfb4c1b249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|    c1|   c3|\n",
      "+------+-----+\n",
      "|0.1234|    b|\n",
      "|0.1234|    b|\n",
      "|0.1234|    b|\n",
      "|1.1234|    b|\n",
      "|2.1234|  b  |\n",
      "|3.1234|     |\n",
      "|4.1234|     |\n",
      "|5.1234| null|\n",
      "|6.1234| null|\n",
      "|7.1234| null|\n",
      "|8.1234| null|\n",
      "|9.1234|     |\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ns.get(\"FAIL_DF_low\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8282a45e-046c-4c5e-9faa-c33e9016e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|     c1|  c2|\n",
      "+-------+----+\n",
      "|10.1234|    |\n",
      "|11.1234|   a|\n",
      "|12.1234|null|\n",
      "|13.1234|null|\n",
      "|14.1234|null|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ns.get(\"FAIL_DF_hi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefc67f-8f77-4d85-8fe4-7e879ee0ce19",
   "metadata": {},
   "source": [
    "### Overwriting keys associated with the failed dataframes\n",
    "\n",
    "The keys used for storing dataframes are generated with a method that prevent any form of overwriting by adding a numerical suffix to them, hence the user should not worry about that.\n",
    "\n",
    "Rerruning the same broken pipeline, without clearing the cache, the keys associated with the failed dataframes do not overwrite the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c52e8968-792a-48ab-8efb-fb08a610d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:05:30,470 | pipelines.py:523 [INFO]: Running *** TransformerPipeline *** (2 transformers) \n",
      "2024-05-16 14:05:30,541 | pipelines.py:556 [INFO]: Running SPLIT <<< hi >>> \n",
      "2024-05-16 14:05:30,542 | pipelines.py:283 [INFO]: Running DropColumns -> PARAMS: columns=\"c3\" ... \n",
      "2024-05-16 14:05:30,547 | pipelines.py:297 [INFO]: Execution time for DropColumns: 0.0s \n",
      "2024-05-16 14:05:30,547 | pipelines.py:556 [INFO]: Running SPLIT <<< low >>> \n",
      "2024-05-16 14:05:30,548 | pipelines.py:283 [INFO]: Running DropColumns -> PARAMS: columns=\"c2\" ... \n",
      "2024-05-16 14:05:30,552 | pipelines.py:297 [INFO]: Execution time for DropColumns: 0.0s \n",
      "2024-05-16 14:05:30,552 | pipelines.py:570 [INFO]: Merging splits: hi, low ... \n",
      "2024-05-16 14:05:30,559 | storage.py:124 [INFO]: Nebula Storage: setting an object (<class 'pyspark.sql.dataframe.DataFrame'>) with the key \"FAIL_DF_hi_0\". \n",
      "2024-05-16 14:05:30,559 | storage.py:124 [INFO]: Nebula Storage: setting an object (<class 'pyspark.sql.dataframe.DataFrame'>) with the key \"FAIL_DF_low_0\". \n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Unable to perform 'unionByName' and append the dataframes. Find the output dataframes of each single split \"hi\", \"low\" in the nebula storage by using the keys \"FAIL_DF_hi_0\", \"FAIL_DF_low_0\" respectively.\nCannot resolve column name \"c2\" among (c1, c3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mpipe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_input\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:1301\u001B[0m, in \u001B[0;36mTransformerPipeline.run\u001B[0;34m(self, df_input)\u001B[0m\n\u001B[1;32m   1298\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m   1299\u001B[0m appending_func \u001B[38;5;241m=\u001B[39m pyspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39munionByName\n\u001B[0;32m-> 1301\u001B[0m df_ret \u001B[38;5;241m=\u001B[39m \u001B[43m_run_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mappending_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m start_time \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1304\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m time()\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:588\u001B[0m, in \u001B[0;36m_run_pipeline\u001B[0;34m(obj, df, appending_func)\u001B[0m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    587\u001B[0m     _, msg, _ \u001B[38;5;241m=\u001B[39m _cache_to_nebula_storage()\n\u001B[0;32m--> 588\u001B[0m     \u001B[43mraise_pipeline_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mrepartition_output_to_original:\n\u001B[1;32m    591\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mrepartition(n_part)\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/exceptions.py:45\u001B[0m, in \u001B[0;36mraise_pipeline_error\u001B[0;34m(e, msg)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m HAS_SPARK \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, CapturedException):\n\u001B[1;32m     44\u001B[0m     e\u001B[38;5;241m.\u001B[39mdesc \u001B[38;5;241m=\u001B[39m msg \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m e\u001B[38;5;241m.\u001B[39mdesc\n\u001B[0;32m---> 45\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(e)(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/gitrepos/nebula/nlsn/nebula/pipelines/pipelines.py:585\u001B[0m, in \u001B[0;36m_run_pipeline\u001B[0;34m(obj, df, appending_func)\u001B[0m\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m obj\u001B[38;5;241m.\u001B[39msplit_cast_to_input_schema:\n\u001B[1;32m    584\u001B[0m         li_df_split \u001B[38;5;241m=\u001B[39m [cast_to_schema(i, input_schema) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m li_df_split]\n\u001B[0;32m--> 585\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mappending_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mli_df_split\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    587\u001B[0m     _, msg, _ \u001B[38;5;241m=\u001B[39m _cache_to_nebula_storage()\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/pyspark/sql/dataframe.py:1901\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n\u001B[1;32m   1858\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other, allowMissingColumns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1859\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   1860\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   1861\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1899\u001B[0m \u001B[38;5;124;03m       missing columns.\u001B[39;00m\n\u001B[1;32m   1900\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1901\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msql_ctx)\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1304\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1305\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1306\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1308\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1309\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1310\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1312\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1313\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m/opt/spark-3.2.0/python/pyspark/sql/utils.py:117\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    113\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Unable to perform 'unionByName' and append the dataframes. Find the output dataframes of each single split \"hi\", \"low\" in the nebula storage by using the keys \"FAIL_DF_hi_0\", \"FAIL_DF_low_0\" respectively.\nCannot resolve column name \"c2\" among (c1, c3)"
     ]
    }
   ],
   "source": [
    "_ = pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b785e5-6958-4583-a0f0-8c0eef371960",
   "metadata": {},
   "source": [
    "The keys associated with the failed dataframes are now:\n",
    "- `FAIL_DF_low_0`\n",
    "- `FAIL_DF_hi_0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849db7d-126d-4f9f-a6e9-bd76d7053c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
