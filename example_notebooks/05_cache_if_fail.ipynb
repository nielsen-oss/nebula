{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f16f8ea-d198-42e5-b0fe-09608bbcc7c7",
   "metadata": {},
   "source": [
    "# Caching the dataframe(s) in case of a pipeline failure\n",
    "\n",
    "This notebook shows a couple of useful features to help the user to debug a broken pipeline.\n",
    "\n",
    "Please note that this debugging process is intended for execution in a notebook, as it relies on nebula storage, which resides within the Python kernel and cannot be utilized in a recipe or Airflow.\n",
    "\n",
    "There two main situations in which a pipeline may break:\n",
    "- a transformer fails\n",
    "- In a split pipeline, the split dataframes become unmergeable due to varying schemas or columns.\n",
    "\n",
    "In the first case nebula stores the input dataframe of the failed transformer, in the latter one all the dataframes that should be merged are retained, allowing the user to retrieve them and address the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fc11c7-5f40-4a82-98f1-a8e7b0cf4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from nebula import nebula_storage as ns\n",
    "from nebula.pipelines.pipelines import TransformerPipeline\n",
    "from nebula.base import Transformer\n",
    "from nebula.transformers import (\n",
    "    AssertNotEmpty,\n",
    "    DropColumns,\n",
    "    RenameColumns,\n",
    "    SelectColumns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0305982f-8523-48fe-8b27-ecddda081949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(TransformerPipeline([]), \"__name__\", None) == \"TransformerPipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cb3a117-bae9-485b-b9eb-1042427f28ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TransformerPipeline'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TransformerPipeline([])).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7718fe-b314-4b9d-b84b-dac0566e57ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00a0d3-4ef3-49d3-993f-be675cb623d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce30c84-88bd-4739-83ec-6d13e83acebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema([('c1', Float64), ('c2', String), ('c3', String)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>c1</th><th>c2</th><th>c3</th></tr><tr><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0.1234</td><td>&quot;a&quot;</td><td>&quot;b&quot;</td></tr><tr><td>4.1234</td><td>&quot;&quot;</td><td>&quot;&quot;</td></tr><tr><td>5.1234</td><td>null</td><td>null</td></tr><tr><td>6.1234</td><td>&quot;&quot;</td><td>null</td></tr><tr><td>8.1234</td><td>&quot;a&quot;</td><td>null</td></tr><tr><td>9.1234</td><td>&quot;a&quot;</td><td>&quot;&quot;</td></tr><tr><td>10.1234</td><td>&quot;&quot;</td><td>&quot;b&quot;</td></tr><tr><td>11.1234</td><td>&quot;a&quot;</td><td>null</td></tr><tr><td>12.1234</td><td>null</td><td>&quot;b&quot;</td></tr><tr><td>14.1234</td><td>&quot;&quot;</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 3)\n",
       "┌─────────┬──────┬──────┐\n",
       "│ c1      ┆ c2   ┆ c3   │\n",
       "│ ---     ┆ ---  ┆ ---  │\n",
       "│ f64     ┆ str  ┆ str  │\n",
       "╞═════════╪══════╪══════╡\n",
       "│ 0.1234  ┆ a    ┆ b    │\n",
       "│ 4.1234  ┆      ┆      │\n",
       "│ 5.1234  ┆ null ┆ null │\n",
       "│ 6.1234  ┆      ┆ null │\n",
       "│ 8.1234  ┆ a    ┆ null │\n",
       "│ 9.1234  ┆ a    ┆      │\n",
       "│ 10.1234 ┆      ┆ b    │\n",
       "│ 11.1234 ┆ a    ┆ null │\n",
       "│ 12.1234 ┆ null ┆ b    │\n",
       "│ 14.1234 ┆      ┆ null │\n",
       "└─────────┴──────┴──────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [4.1234, \"\", \"\"],\n",
    "    [5.1234, None, None],\n",
    "    [6.1234, \"\", None],\n",
    "    [8.1234, \"a\", None],\n",
    "    [9.1234, \"a\", \"\"],\n",
    "    [10.1234, \"\", \"b\"],\n",
    "    [11.1234, \"a\", None],\n",
    "    [12.1234, None, \"b\"],\n",
    "    [14.1234, \"\", None],\n",
    "]\n",
    "\n",
    "df_input = pl.DataFrame(data, orient=\"row\", schema=[\"c1\", \"c2\", \"c3\"])\n",
    "print(df_input.schema)\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2166b6ba-efda-4433-8eb5-ac933943873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_function_with_null(df: pl.DataFrame) -> dict[str, pl.DataFrame]:\n",
    "    \"\"\"Split dataframe into 'low', 'hi', and 'null' subsets.\"\"\"\n",
    "    ret = _split_function(df)\n",
    "    # Include both actual nulls and NaN values in the 'null' split\n",
    "    cond_null = pl.col(\"c1\").is_null() | pl.col(\"c1\").is_nan()\n",
    "    return {**ret, \"null\": df.filter(cond_null)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b0bcef-5561-4338-8994-094a3ff2b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_transformers = {\"low\": [], \"hi\": []}\n",
    "\n",
    "pipe = TransformerPipeline(\n",
    "    dict_transformers,\n",
    "    split_function=_split_function_with_null,\n",
    "    splits_no_merge={\"hi\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f414e36-652d-4cf6-aec3-e106f1b46ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_config',\n",
       " '_ir',\n",
       " '_should_skip',\n",
       " 'find_node',\n",
       " 'get_node_ids',\n",
       " 'get_number_transformers',\n",
       " 'name',\n",
       " 'plot',\n",
       " 'run',\n",
       " 'show',\n",
       " 'to_string']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b55ee1e4-d59f-4fef-b7c0-8172bd82302c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InputNode(id='input_88f438@0', node_type=<NodeType.INPUT: 1>, position=(0,), metadata={'name': 'DF input'}, children=[], name='DF input'),\n",
       " ForkNode(id='fork_split_b4f1d2@1', node_type=<NodeType.FORK: 6>, position=(1,), metadata={}, children=[], fork_type='split', config={'splits_no_merge': {'hi'}, 'splits_skip_if_empty': set(), 'cast_subsets_to_input_schema': False, 'repartition_output_to_original': False, 'coalesce_output_to_original': False}, branches={'hi': [], 'low': []}, otherwise=None, split_function=<function _split_function_with_null at 0x0000019BFF2C1090>),\n",
       " MergeNode(id='merge_append_c29f49@2', node_type=<NodeType.MERGE: 7>, position=(2,), metadata={}, children=[], merge_type='append', config={'allow_missing_columns': False, 'cast_subsets_to_input_schema': False, 'repartition_output_to_original': False, 'coalesce_output_to_original': False, 'dead_end_splits': {'hi'}}),\n",
       " OutputNode(id='output_2a4e73@3', node_type=<NodeType.OUTPUT: 2>, position=(3,), metadata={'name': 'DF output'}, children=[], name='DF output')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe._ir.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64598d83-b06a-4b09-b99f-a87d0600f156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da247af-df3a-41aa-879b-0abcfe38bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    StructField(\"c1\", FloatType(), True),\n",
    "    StructField(\"c2\", StringType(), True),\n",
    "    StructField(\"c3\", StringType(), True),\n",
    "]\n",
    "\n",
    "data = [\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [0.1234, \"a\", \"b\"],\n",
    "    [1.1234, \"a\", \"  b\"],\n",
    "    [2.1234, \"  a  \", \"  b  \"],\n",
    "    [3.1234, \"\", \"\"],\n",
    "    [4.1234, \"   \", \"   \"],\n",
    "    [5.1234, None, None],\n",
    "    [6.1234, \" \", None],\n",
    "    [7.1234, \"\", None],\n",
    "    [8.1234, \"a\", None],\n",
    "    [9.1234, \"a\", \"\"],\n",
    "    [10.1234, \"   \", \"b\"],\n",
    "    [11.1234, \"a\", None],\n",
    "    [12.1234, None, \"b\"],\n",
    "    [13.1234, None, \"b\"],\n",
    "    [14.1234, None, None],\n",
    "]\n",
    "\n",
    "df_input = spark.createDataFrame(data, schema=StructType(schema)).cache()\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f913e-e8f7-4891-88b4-569fda9e946a",
   "metadata": {},
   "source": [
    "## Transformer failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d109f1-056a-4e4d-8c1a-1b1b84eebe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThisTransformerIsBroken:\n",
    "    @staticmethod\n",
    "    def transform(df):\n",
    "        \"\"\"Public transform method w/o parent class.\"\"\"        \n",
    "        return df.select(\"wrong\")\n",
    "\n",
    "\n",
    "# clear the cache\n",
    "ns.clear()\n",
    "\n",
    "pipe = TransformerPipeline([\n",
    "    NanToNull(input_columns=\"*\"),\n",
    "    ThisTransformerIsBroken(),\n",
    "    Distinct(),\n",
    "])\n",
    "\n",
    "pipe.show_pipeline(add_transformer_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016febd-a9b7-482e-bb0a-3cca7c6585aa",
   "metadata": {},
   "source": [
    "### Retrieve the input dataframe of the failed transformer as the pipe breaks.\n",
    "\n",
    "The error message will contain the key(s) associated with storing the aforementioned dataframe. \n",
    "\n",
    "A few lines above, the original exception is documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b96c7-db3f-4490-98bd-d820cf017fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d0a64-b046-4d7f-89c9-71fccafa10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.get(\"FAIL_DF_ThisTransformerIsBroken\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ba961-635e-4674-86b7-8c8c67d3ff3a",
   "metadata": {},
   "source": [
    "## Unable to merge splits\n",
    "\n",
    "In this example the transformers work properly, but they modified the dataframes in a way that is not possible to merge them back anymore.\n",
    "\n",
    "To address this issue, all the dataframes before the union process are stored, allowing the user to investigate the problem.\n",
    "\n",
    "In this example one split drops the column `c2`, the other one the column `c3`, hence they cannot be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff404d-0438-4d90-bd11-7829e38c0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split_function(df):\n",
    "    cond = F.col(\"c1\") < 10\n",
    "    return {\n",
    "        \"low\": df.filter(cond),\n",
    "        \"hi\": df.filter(~cond),\n",
    "    }\n",
    "\n",
    "\n",
    "dict_transf = {\n",
    "    \"low\": [DropColumns(columns=\"c2\")],\n",
    "    \"hi\": [DropColumns(columns=\"c3\")],\n",
    "}\n",
    "\n",
    "# clear the cache\n",
    "ns.clear()\n",
    "\n",
    "pipe = TransformerPipeline(dict_transf, split_function=my_split_function)\n",
    "\n",
    "pipe.show_pipeline(add_transformer_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576726f-8d54-4cc4-8625-7619bb6efda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789f772-d861-465a-b046-fcfb4c1b249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.get(\"FAIL_DF_low\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282a45e-046c-4c5e-9faa-c33e9016e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.get(\"FAIL_DF_hi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefc67f-8f77-4d85-8fe4-7e879ee0ce19",
   "metadata": {},
   "source": [
    "### Overwriting keys associated with the failed dataframes\n",
    "\n",
    "The keys used for storing dataframes are generated with a method that prevent any form of overwriting by adding a numerical suffix to them, hence the user should not worry about that.\n",
    "\n",
    "Rerruning the same broken pipeline, without clearing the cache, the keys associated with the failed dataframes do not overwrite the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e8968-792a-48ab-8efb-fb08a610d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pipe.run(df_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b785e5-6958-4583-a0f0-8c0eef371960",
   "metadata": {},
   "source": [
    "The keys associated with the failed dataframes are now:\n",
    "- `FAIL_DF_low_0`\n",
    "- `FAIL_DF_hi_0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849db7d-126d-4f9f-a6e9-bd76d7053c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
