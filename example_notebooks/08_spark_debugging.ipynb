{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 08 - Spark: Debugging & Optimization\n",
    "\n",
    "This notebook covers Spark-specific utilities for debugging and optimizing pipelines.\n",
    "\n",
    "| Part | Topic |\n",
    "|------|-------|\n",
    "| **1** | CpuInfo - Know Your Workers |\n",
    "| **2** | LogDataSkew - Monitor Partition Distribution |\n",
    "| **3** | Detecting Skew in Split Pipelines |\n",
    "| **4** | Worker Package Diagnostics |\n",
    "\n",
    "‚ö†Ô∏è **This notebook requires a Spark session.** Examples show expected output but won't run without PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session setup (adjust for your environment)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Nebula Spark Demo\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from nebula import TransformerPipeline\n",
    "from nebula.transformers.spark_transformers import CpuInfo, LogDataSkew, Repartition\n",
    "from nebula.transformers import AddLiterals, Filter, SelectColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with 10K rows\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.range(10000).withColumn(\n",
    "    \"category\", \n",
    "    F.when(F.col(\"id\") < 100, \"tiny\")      # 100 rows (1%)\n",
    "     .when(F.col(\"id\") < 500, \"small\")     # 400 rows (4%)\n",
    "     .when(F.col(\"id\") < 2000, \"medium\")   # 1500 rows (15%)\n",
    "     .otherwise(\"large\")                    # 8000 rows (80%)\n",
    ").withColumn(\n",
    "    \"value\", (F.rand() * 1000).cast(\"int\")\n",
    ")\n",
    "\n",
    "df = df.repartition(8)  # Start with 8 balanced partitions\n",
    "df.groupBy(\"category\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: CpuInfo - Know Your Workers\n",
    "\n",
    "`CpuInfo` runs a UDF across workers to report their CPU model. This is useful when:\n",
    "\n",
    "- You're on a **managed cluster** (Databricks, EMR, Dataproc) and want to verify hardware\n",
    "- You suspect **heterogeneous workers** are causing performance issues\n",
    "- You want to **blame infrastructure** when things are slow üòÑ\n",
    "\n",
    "### 1.1 Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cpuinfo-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once at pipeline start to see what you're working with\n",
    "cpu_checker = CpuInfo()\n",
    "df_with_cpu = cpu_checker.transform(df)\n",
    "\n",
    "# The transformer adds no columns - it just logs CPU info\n",
    "# Check your Spark logs for output like:\n",
    "# \n",
    "# *** CpuInfo ***\n",
    "# Intel(R) Xeon(R) CPU @ 2.20GHz: 4 workers\n",
    "# AMD EPYC 7B12: 2 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cpuinfo-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically placed at the start of a pipeline\n",
    "pipe = TransformerPipeline(\n",
    "    [\n",
    "        CpuInfo(),  # First thing: what hardware do we have?\n",
    "        SelectColumns(columns=[\"id\", \"category\", \"value\"]),\n",
    "        # ... rest of pipeline\n",
    "    ],\n",
    "    name=\"Pipeline with CPU Check\",\n",
    ")\n",
    "\n",
    "pipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: LogDataSkew - Monitor Partition Distribution\n",
    "\n",
    "`LogDataSkew` reports how rows are distributed across partitions. Skewed partitions are a common cause of slow Spark jobs - one task takes forever while others finish quickly.\n",
    "\n",
    "### 2.1 Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skew-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check partition distribution\n",
    "skew_checker = LogDataSkew()\n",
    "df_checked = skew_checker.transform(df)\n",
    "\n",
    "# Output in logs:\n",
    "# \n",
    "# *** LogDataSkew ***\n",
    "# Partitions: 8\n",
    "# Total rows: 10000\n",
    "# Min/Max rows per partition: 1200 / 1300\n",
    "# Skew ratio: 1.08x (healthy < 2x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interleaved-header",
   "metadata": {},
   "source": [
    "### 2.2 As Interleaved Transformer\n",
    "\n",
    "The real power comes from using `LogDataSkew` as an **interleaved transformer** - it runs after every step, showing how skew evolves through your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skew-interleaved",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor skew at every step\n",
    "pipe = TransformerPipeline(\n",
    "    [\n",
    "        SelectColumns(columns=[\"id\", \"category\", \"value\"]),\n",
    "        Filter(input_col=\"value\", perform=\"keep\", operator=\"gt\", value=100),\n",
    "        AddLiterals(data=[{\"alias\": \"processed\", \"value\": True}]),\n",
    "    ],\n",
    "    name=\"Pipeline with Skew Monitoring\",\n",
    "    interleaved=[LogDataSkew()],  # Runs after each transformer\n",
    ")\n",
    "\n",
    "pipe.show()\n",
    "\n",
    "# When you run this, logs show skew after EACH step:\n",
    "# Running 'SelectColumns' ...\n",
    "# *** LogDataSkew *** Partitions: 8, Skew: 1.08x\n",
    "# Running 'Filter' ...\n",
    "# *** LogDataSkew *** Partitions: 8, Skew: 1.12x\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "force-interleaved",
   "metadata": {},
   "source": [
    "### 2.3 Runtime Injection with `force_interleaved_transformer`\n",
    "\n",
    "Don't want to modify your pipeline definition? Inject `LogDataSkew` at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "force-interleaved-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original pipeline - no skew monitoring\n",
    "production_pipe = TransformerPipeline(\n",
    "    [\n",
    "        SelectColumns(columns=[\"id\", \"category\", \"value\"]),\n",
    "        Filter(input_col=\"value\", perform=\"keep\", operator=\"gt\", value=100),\n",
    "    ],\n",
    "    name=\"Production Pipeline\",\n",
    ")\n",
    "\n",
    "# Inject skew monitoring at runtime (for debugging)\n",
    "result = production_pipe.run(\n",
    "    df,\n",
    "    force_interleaved_transformer=LogDataSkew(),\n",
    ")\n",
    "\n",
    "# Production code stays clean, debugging when needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Detecting Skew in Split Pipelines\n",
    "\n",
    "Split pipelines are a common source of skew - when you split data by category and then append, partition distribution often becomes unbalanced.\n",
    "\n",
    "### 3.1 Creating a Skewed Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def split_by_category(df):\n",
    "    \"\"\"Split into 4 very unequal subsets.\"\"\"\n",
    "    return {\n",
    "        \"tiny\": df.filter(F.col(\"category\") == \"tiny\"),      # 100 rows (1%)\n",
    "        \"small\": df.filter(F.col(\"category\") == \"small\"),    # 400 rows (4%)\n",
    "        \"medium\": df.filter(F.col(\"category\") == \"medium\"),  # 1500 rows (15%)\n",
    "        \"large\": df.filter(F.col(\"category\") == \"large\"),    # 8000 rows (80%)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skewed-split-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_pipeline = TransformerPipeline(\n",
    "    {\n",
    "        \"tiny\": [\n",
    "            AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"critical\"}]),\n",
    "        ],\n",
    "        \"small\": [\n",
    "            AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"high\"}]),\n",
    "        ],\n",
    "        \"medium\": [\n",
    "            AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"normal\"}]),\n",
    "        ],\n",
    "        \"large\": [\n",
    "            AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"batch\"}]),\n",
    "        ],\n",
    "    },\n",
    "    split_function=split_by_category,\n",
    "    name=\"Category Processing\",\n",
    ")\n",
    "\n",
    "skewed_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring-skew",
   "metadata": {},
   "source": [
    "### 3.2 Monitoring the Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-monitoring-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = TransformerPipeline(\n",
    "    [\n",
    "        CpuInfo(),  # What hardware?\n",
    "        \n",
    "        # This split will create skew\n",
    "        skewed_pipeline,\n",
    "        \n",
    "        # After append, partitions are unbalanced\n",
    "        SelectColumns(columns=[\"id\", \"category\", \"value\", \"priority\"]),\n",
    "    ],\n",
    "    name=\"Full Pipeline with Skew\",\n",
    ")\n",
    "\n",
    "# Run with skew monitoring\n",
    "result = full_pipeline.run(\n",
    "    df,\n",
    "    force_interleaved_transformer=LogDataSkew(),\n",
    ")\n",
    "\n",
    "# Expected log output:\n",
    "#\n",
    "# Running 'CpuInfo' ...\n",
    "# *** CpuInfo *** Intel Xeon: 4 workers\n",
    "# *** LogDataSkew *** Partitions: 8, Skew: 1.08x ‚úì\n",
    "#\n",
    "# Entering split ...\n",
    "# ... (split processing) ...\n",
    "# <<< Append DFs >>>\n",
    "# *** LogDataSkew *** Partitions: 32, Skew: 80x ‚ö†Ô∏è  <- PROBLEM!\n",
    "#\n",
    "# Running 'SelectColumns' ...\n",
    "# *** LogDataSkew *** Partitions: 32, Skew: 80x ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixing-skew",
   "metadata": {},
   "source": [
    "### 3.3 Fixing the Skew\n",
    "\n",
    "Use `repartition_output_to_original` or `coalesce_output_to_original`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_pipeline = TransformerPipeline(\n",
    "    {\n",
    "        \"tiny\": [AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"critical\"}])],\n",
    "        \"small\": [AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"high\"}])],\n",
    "        \"medium\": [AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"normal\"}])],\n",
    "        \"large\": [AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"batch\"}])],\n",
    "    },\n",
    "    split_function=split_by_category,\n",
    "    name=\"Category Processing (Fixed)\",\n",
    "    repartition_output_to_original=True,  # Rebalance after append\n",
    ")\n",
    "\n",
    "# Now skew is fixed after the split\n",
    "# *** LogDataSkew *** Partitions: 8, Skew: 1.25x ‚úì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Worker Package Diagnostics\n",
    "\n",
    "When you don't manage the Spark cluster directly (Databricks, EMR, company clusters), package version mismatches between driver and workers cause cryptic errors.\n",
    "\n",
    "### 4.1 Check if a Package is Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lib-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebula.spark_udfs import lib_in_spark_workers, lib_version_in_spark_workers\n",
    "\n",
    "# Check if pandas is available on workers\n",
    "has_pandas = lib_in_spark_workers(spark, \"pandas\")\n",
    "print(f\"pandas installed on workers: {has_pandas}\")\n",
    "\n",
    "# Check a package that might not be there\n",
    "has_xgboost = lib_in_spark_workers(spark, \"xgboost\")\n",
    "print(f\"xgboost installed on workers: {has_xgboost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "version-check",
   "metadata": {},
   "source": [
    "### 4.2 Check Package Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "version-check-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the version installed on workers\n",
    "pandas_version = lib_version_in_spark_workers(spark, \"pandas\")\n",
    "print(f\"Worker pandas version: {pandas_version}\")\n",
    "\n",
    "# Compare with driver\n",
    "import pandas as pd\n",
    "print(f\"Driver pandas version: {pd.__version__}\")\n",
    "\n",
    "# Mismatch? That might explain your UDF errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-pattern",
   "metadata": {},
   "source": [
    "### 4.3 Diagnostic Pattern\n",
    "\n",
    "When debugging cluster issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_cluster(spark, packages: list[str]):\n",
    "    \"\"\"Quick cluster diagnostic.\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"CLUSTER DIAGNOSTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for pkg in packages:\n",
    "        installed = lib_in_spark_workers(spark, pkg)\n",
    "        if installed:\n",
    "            version = lib_version_in_spark_workers(spark, pkg)\n",
    "            print(f\"‚úì {pkg}: {version}\")\n",
    "        else:\n",
    "            print(f\"‚úó {pkg}: NOT INSTALLED\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Usage\n",
    "diagnose_cluster(spark, [\"pandas\", \"numpy\", \"pyarrow\", \"nebula\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Tool | Purpose | When to Use |\n",
    "|------|---------|-------------|\n",
    "| `CpuInfo` | Report worker CPU types | Start of pipeline, debugging performance |\n",
    "| `LogDataSkew` | Monitor partition distribution | Interleaved or after joins/splits |\n",
    "| `lib_in_spark_workers` | Check package availability | Debugging UDF failures |\n",
    "| `lib_version_in_spark_workers` | Check package versions | Driver/worker version mismatch |\n",
    "\n",
    "**Skew Prevention:**\n",
    "- Use `repartition_output_to_original=True` on split pipelines\n",
    "- Monitor with `LogDataSkew` as interleaved transformer\n",
    "- Inject `force_interleaved_transformer=LogDataSkew()` at runtime for debugging\n",
    "\n",
    "**Cluster Debugging:**\n",
    "- When UDFs fail mysteriously, check package versions\n",
    "- Driver version ‚â† worker version is a common issue on managed clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
