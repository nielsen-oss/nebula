{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 08 - Spark: Debugging & Optimization\n",
    "\n",
    "This notebook covers Spark-specific utilities for debugging and optimizing pipelines.\n",
    "\n",
    "| Part | Topic |\n",
    "|------|-------|\n",
    "| **1** | CpuInfo - Know Your Workers |\n",
    "| **2** | LogDataSkew - Monitor Partition Distribution |\n",
    "| **3** | Detecting Skew in Split Pipelines |\n",
    "| **4** | Worker Package Diagnostics |\n",
    "\n",
    "‚ö†Ô∏è **This notebook requires a Spark session.** Examples show expected output but won't run without PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/27 16:00:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark session setup (adjust for your environment)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Nebula Spark Demo\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from nebula import TransformerPipeline\n",
    "from nebula.transformers.spark_transformers import CpuInfo, LogDataSkew, Repartition\n",
    "from nebula.transformers import AddLiterals, Filter, SelectColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sample-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|    tiny|  100|\n",
      "|  medium| 1500|\n",
      "|   small|  400|\n",
      "|   large| 8000|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Create sample data with 10K rows\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.range(10000).withColumn(\n",
    "    \"category\", \n",
    "    F.when(F.col(\"id\") < 100, \"tiny\")      # 100 rows (1%)\n",
    "     .when(F.col(\"id\") < 500, \"small\")     # 400 rows (4%)\n",
    "     .when(F.col(\"id\") < 2000, \"medium\")   # 1500 rows (15%)\n",
    "     .otherwise(\"large\")                    # 8000 rows (80%)\n",
    ").withColumn(\n",
    "    \"value\", (F.rand() * 1000).cast(\"int\")\n",
    ")\n",
    "\n",
    "df = df.repartition(8)  # Start with 8 balanced partitions\n",
    "df.groupBy(\"category\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: CpuInfo - Know Your Workers\n",
    "\n",
    "It requires `cpuinfo`, `pip install py-cpuinfo`.\n",
    "\n",
    "`CpuInfo` runs a UDF across workers to report their CPU model. This is useful when:\n",
    "\n",
    "- You're on a **managed cluster** (Databricks, EMR, Dataproc) and want to verify hardware\n",
    "- You suspect **heterogeneous workers** are causing performance issues\n",
    "- You want to **blame infrastructure** when things are slow üòÑ\n",
    "\n",
    "### 1.1 Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cpuinfo-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vamarett2101/Desktop/oss/nebula/venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "[Stage 8:===================================================>    (92 + 4) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+---------------+\n",
      "|cpu                                     |host           |\n",
      "+----------------------------------------+---------------+\n",
      "|Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz|CHLC02FN46PMD6T|\n",
      "+----------------------------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Run once at pipeline start to see what you're working with\n",
    "cpu_checker = CpuInfo()\n",
    "df_with_cpu = cpu_checker.transform(df)\n",
    "\n",
    "# The transformer adds no columns - it just logs CPU info\n",
    "# Check your Spark logs for output like:\n",
    "# \n",
    "# *** CpuInfo ***\n",
    "# Intel(R) Xeon(R) CPU @ 2.20GHz: 4 workers\n",
    "# AMD EPYC 7B12: 2 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cpuinfo-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline with CPU Check *** (2 transformations)\n",
      " - CpuInfo\n",
      " - SelectColumns\n"
     ]
    }
   ],
   "source": [
    "# Typically placed at the start of a pipeline\n",
    "pipe = TransformerPipeline(\n",
    "    [\n",
    "        CpuInfo(),  # First thing: what hardware do we have?\n",
    "        SelectColumns(columns=[\"id\", \"category\", \"value\"]),\n",
    "        # ... rest of pipeline\n",
    "    ],\n",
    "    name=\"Pipeline with CPU Check\",\n",
    ")\n",
    "\n",
    "pipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: LogDataSkew - Monitor Partition Distribution\n",
    "\n",
    "`LogDataSkew` reports how rows are distributed across partitions. Skewed partitions are a common cause of slow Spark jobs - one task takes forever while others finish quickly.\n",
    "\n",
    "### 2.1 Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "skew-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 16:01:28,218 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:01:28,219 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:01:28,219 | [INFO]: mean=1,250.0 | std=0.8 | min=1,249.0 | 25%=1,249.8 | 50%=1,250.0 | 75%=1,250.2 | max=1,251.0 \n"
     ]
    }
   ],
   "source": [
    "# Check partition distribution\n",
    "skew_checker = LogDataSkew()\n",
    "df_checked = skew_checker.transform(df)\n",
    "\n",
    "# Output in logs:\n",
    "# \n",
    "# *** LogDataSkew ***\n",
    "# Partitions: 8\n",
    "# Total rows: 10000\n",
    "# Min/Max rows per partition: 1200 / 1300\n",
    "# Skew ratio: 1.08x (healthy < 2x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interleaved-header",
   "metadata": {},
   "source": [
    "### 2.2 As Interleaved Transformer\n",
    "\n",
    "The real power comes from using `LogDataSkew` as an **interleaved transformer** - it runs after every step, showing how skew evolves through your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "skew-interleaved",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 16:01:28,226 | [INFO]: Starting pipeline 'Pipeline with Skew Monitoring' \n",
      "2025-12-27 16:01:28,227 | [INFO]: Running 'SelectColumns' ... \n",
      "2025-12-27 16:01:28,249 | [INFO]: Completed 'SelectColumns' in 0.0s \n",
      "2025-12-27 16:01:28,250 | [INFO]: Running 'LogDataSkew' ... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline with Skew Monitoring *** (5 transformations)\n",
      " - SelectColumns\n",
      " - LogDataSkew\n",
      " - Filter\n",
      " - LogDataSkew\n",
      " - AddLiterals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 16:01:28,610 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:01:28,611 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:01:28,612 | [INFO]: mean=1,250.0 | std=0.8 | min=1,249.0 | 25%=1,249.8 | 50%=1,250.0 | 75%=1,250.2 | max=1,251.0 \n",
      "2025-12-27 16:01:28,612 | [INFO]: Completed 'LogDataSkew' in 0.4s \n",
      "2025-12-27 16:01:28,613 | [INFO]: Running 'Filter' ... \n",
      "2025-12-27 16:01:28,625 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-27 16:01:28,625 | [INFO]: Running 'LogDataSkew' ... \n",
      "2025-12-27 16:01:29,041 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:01:29,042 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:01:29,043 | [INFO]: mean=1,119.8 | std=0.5 | min=1,119.0 | 25%=1,119.8 | 50%=1,120.0 | 75%=1,120.0 | max=1,120.0 \n",
      "2025-12-27 16:01:29,043 | [INFO]: Completed 'LogDataSkew' in 0.4s \n",
      "2025-12-27 16:01:29,044 | [INFO]: Running 'AddLiterals' ... \n",
      "2025-12-27 16:01:29,054 | [INFO]: Completed 'AddLiterals' in 0.0s \n",
      "2025-12-27 16:01:29,054 | [INFO]: Pipeline 'Pipeline with Skew Monitoring' completed in 0.8s \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, category: string, value: int, processed: boolean]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monitor skew at every step\n",
    "pipe = TransformerPipeline(\n",
    "    [\n",
    "        SelectColumns(columns=[\"id\", \"category\", \"value\"]),\n",
    "        Filter(input_col=\"value\", perform=\"keep\", operator=\"gt\", value=100),\n",
    "        AddLiterals(data=[{\"alias\": \"processed\", \"value\": True}]),\n",
    "    ],\n",
    "    name=\"Pipeline with Skew Monitoring\",\n",
    "    interleaved=[LogDataSkew()],  # Runs after each transformer\n",
    ")\n",
    "\n",
    "pipe.show()\n",
    "\n",
    "# When you run this, logs show skew after EACH step:\n",
    "# Running 'SelectColumns' ...\n",
    "# *** LogDataSkew *** Partitions: 8, Skew: 1.08x\n",
    "# Running 'Filter' ...\n",
    "# *** LogDataSkew *** Partitions: 8, Skew: 1.12x\n",
    "# ...\n",
    "pipe.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "force-interleaved",
   "metadata": {},
   "source": [
    "### 2.3 Runtime Injection with `force_interleaved_transformer`\n",
    "\n",
    "Don't want to modify your pipeline definition? Inject `LogDataSkew` at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "force-interleaved-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 16:01:29,067 | [INFO]: Starting pipeline 'Production Pipeline' \n",
      "2025-12-27 16:01:29,069 | [INFO]: Running 'SelectColumns' ... \n",
      "2025-12-27 16:01:29,390 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:01:29,391 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:01:29,391 | [INFO]: mean=1,250.0 | std=0.8 | min=1,249.0 | 25%=1,249.8 | 50%=1,250.0 | 75%=1,250.2 | max=1,251.0 \n",
      "2025-12-27 16:01:29,392 | [INFO]: Completed 'SelectColumns' in 0.3s \n",
      "2025-12-27 16:01:29,393 | [INFO]: Running 'Filter' ... \n",
      "2025-12-27 16:01:29,720 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:01:29,721 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:01:29,722 | [INFO]: mean=1,119.8 | std=0.5 | min=1,119.0 | 25%=1,119.8 | 50%=1,120.0 | 75%=1,120.0 | max=1,120.0 \n",
      "2025-12-27 16:01:29,722 | [INFO]: Completed 'Filter' in 0.3s \n",
      "2025-12-27 16:01:29,723 | [INFO]: Pipeline 'Production Pipeline' completed in 0.7s \n"
     ]
    }
   ],
   "source": [
    "# Original pipeline - no skew monitoring\n",
    "production_pipe = TransformerPipeline(\n",
    "    [\n",
    "        SelectColumns(columns=[\"id\", \"category\", \"value\"]),\n",
    "        Filter(input_col=\"value\", perform=\"keep\", operator=\"gt\", value=100),\n",
    "    ],\n",
    "    name=\"Production Pipeline\",\n",
    ")\n",
    "\n",
    "# Inject skew monitoring at runtime (for debugging)\n",
    "result = production_pipe.run(\n",
    "    df,\n",
    "    force_interleaved_transformer=LogDataSkew(),\n",
    ")\n",
    "\n",
    "# Production code stays clean, debugging when needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Detecting Skew in Split Pipelines\n",
    "\n",
    "Split pipelines are a common source of skew - when you split data by category and then append, partition distribution often becomes unbalanced.\n",
    "\n",
    "### 3.1 Creating a Skewed Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "split-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def split_by_category(df):\n",
    "    \"\"\"Split into 4 very unequal subsets.\"\"\"\n",
    "    return {\n",
    "        \"tiny\": df.filter(F.col(\"category\") == \"tiny\"),      # 100 rows (1%)\n",
    "        \"small\": df.filter(F.col(\"category\") == \"small\"),    # 400 rows (4%)\n",
    "        \"medium\": df.filter(F.col(\"category\") == \"medium\"),  # 1500 rows (15%)\n",
    "        \"large\": df.filter(F.col(\"category\") == \"large\"),    # 8000 rows (80%)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "skewed-split-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Category Processing *** (4 transformations)\n",
      "------ SPLIT ------ (function: split_by_category)\n",
      "**SPLIT <<< large >>> (1 transformation):\n",
      "     - AddLiterals\n",
      "**SPLIT <<< medium >>> (1 transformation):\n",
      "     - AddLiterals\n",
      "**SPLIT <<< small >>> (1 transformation):\n",
      "     - AddLiterals\n",
      "**SPLIT <<< tiny >>> (1 transformation):\n",
      "     - AddLiterals\n",
      "<<< Append DFs >>>\n"
     ]
    }
   ],
   "source": [
    "skewed_pipeline = TransformerPipeline(\n",
    "    {\n",
    "        \"tiny\": [\n",
    "            AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"critical\"}]),\n",
    "        ],\n",
    "        \"small\": [\n",
    "            AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"high\"}]),\n",
    "        ],\n",
    "        \"medium\": [\n",
    "            AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"normal\"}]),\n",
    "        ],\n",
    "        \"large\": [\n",
    "            AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"batch\"}]),\n",
    "        ],\n",
    "    },\n",
    "    split_function=split_by_category,\n",
    "    name=\"Category Processing\",\n",
    ")\n",
    "\n",
    "skewed_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring-skew",
   "metadata": {},
   "source": [
    "### 3.2 Monitoring the Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "full-monitoring-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 16:01:29,744 | [INFO]: Starting pipeline 'Full Pipeline with Skew' \n",
      "2025-12-27 16:01:29,745 | [INFO]: Running 'CpuInfo' ... \n",
      "/Users/vamarett2101/Desktop/oss/nebula/venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+---------------+\n",
      "|cpu                                     |host           |\n",
      "+----------------------------------------+---------------+\n",
      "|Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz|CHLC02FN46PMD6T|\n",
      "+----------------------------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 16:02:11,999 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:02:12,000 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:02:12,001 | [INFO]: mean=1,250.0 | std=0.8 | min=1,249.0 | 25%=1,249.8 | 50%=1,250.0 | 75%=1,250.2 | max=1,251.0 \n",
      "2025-12-27 16:02:12,001 | [INFO]: Completed 'CpuInfo' in 42.3s \n",
      "2025-12-27 16:02:12,002 | [INFO]: Entering split \n",
      "2025-12-27 16:02:12,020 | [INFO]: Running 'AddLiterals' ... \n",
      "2025-12-27 16:02:12,374 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:02:12,374 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:02:12,375 | [INFO]: mean=1,000.0 | std=0.8 | min=999.0 | 25%=999.8 | 50%=1,000.0 | 75%=1,000.2 | max=1,001.0 \n",
      "2025-12-27 16:02:12,377 | [INFO]: Completed 'AddLiterals' in 0.4s \n",
      "2025-12-27 16:02:12,379 | [INFO]: Running 'AddLiterals' ... \n",
      "2025-12-27 16:02:12,730 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:02:12,730 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:02:12,731 | [INFO]: mean=187.5 | std=0.5 | min=187.0 | 25%=187.0 | 50%=187.5 | 75%=188.0 | max=188.0 \n",
      "2025-12-27 16:02:12,731 | [INFO]: Completed 'AddLiterals' in 0.4s \n",
      "2025-12-27 16:02:12,732 | [INFO]: Running 'AddLiterals' ... \n",
      "2025-12-27 16:02:13,045 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:02:13,046 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:02:13,046 | [INFO]: mean=50.0 | std=0.0 | min=50.0 | 25%=50.0 | 50%=50.0 | 75%=50.0 | max=50.0 \n",
      "2025-12-27 16:02:13,047 | [INFO]: Completed 'AddLiterals' in 0.3s \n",
      "2025-12-27 16:02:13,048 | [INFO]: Running 'AddLiterals' ... \n",
      "2025-12-27 16:02:13,361 | [INFO]: Number of partitions: 8 \n",
      "2025-12-27 16:02:13,362 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:02:13,362 | [INFO]: mean=12.5 | std=0.5 | min=12.0 | 25%=12.0 | 50%=12.5 | 75%=13.0 | max=13.0 \n",
      "2025-12-27 16:02:13,363 | [INFO]: Completed 'AddLiterals' in 0.3s \n",
      "2025-12-27 16:02:14,564 | [INFO]: Number of partitions: 32                      \n",
      "2025-12-27 16:02:14,565 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:02:14,565 | [INFO]: mean=312.5 | std=408.7 | min=12.0 | 25%=40.8 | 50%=118.5 | 75%=390.8 | max=1,001.0 \n",
      "2025-12-27 16:02:14,566 | [INFO]: Running 'SelectColumns' ... \n",
      "2025-12-27 16:02:15,713 | [INFO]: Number of partitions: 32                      \n",
      "2025-12-27 16:02:15,714 | [INFO]: Rows distribution in partitions: \n",
      "2025-12-27 16:02:15,715 | [INFO]: mean=312.5 | std=408.7 | min=12.0 | 25%=40.8 | 50%=118.5 | 75%=390.8 | max=1,001.0 \n",
      "2025-12-27 16:02:15,717 | [INFO]: Completed 'SelectColumns' in 1.2s \n",
      "2025-12-27 16:02:15,718 | [INFO]: Pipeline 'Full Pipeline with Skew' completed in 46.0s \n"
     ]
    }
   ],
   "source": [
    "full_pipeline = TransformerPipeline(\n",
    "    [\n",
    "        CpuInfo(),  # What hardware?\n",
    "        \n",
    "        # This split will create skew\n",
    "        skewed_pipeline,\n",
    "        \n",
    "        # After append, partitions are unbalanced\n",
    "        SelectColumns(columns=[\"id\", \"category\", \"value\", \"priority\"]),\n",
    "    ],\n",
    "    name=\"Full Pipeline with Skew\",\n",
    ")\n",
    "\n",
    "# Run with skew monitoring\n",
    "result = full_pipeline.run(\n",
    "    df,\n",
    "    force_interleaved_transformer=LogDataSkew(),\n",
    ")\n",
    "\n",
    "# Expected log output:\n",
    "#\n",
    "# Running 'CpuInfo' ...\n",
    "# *** CpuInfo *** Intel Xeon: 4 workers\n",
    "# *** LogDataSkew *** Partitions: 8, Skew: 1.08x ‚úì\n",
    "#\n",
    "# Entering split ...\n",
    "# ... (split processing) ...\n",
    "# <<< Append DFs >>>\n",
    "# *** LogDataSkew *** Partitions: 32, Skew: 80x ‚ö†Ô∏è  <- PROBLEM!\n",
    "#\n",
    "# Running 'SelectColumns' ...\n",
    "# *** LogDataSkew *** Partitions: 32, Skew: 80x ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixing-skew",
   "metadata": {},
   "source": [
    "### 3.3 Fixing the Skew\n",
    "\n",
    "Use `repartition_output_to_original` or `coalesce_output_to_original`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fixed-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_pipeline = TransformerPipeline(\n",
    "    {\n",
    "        \"tiny\": [AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"critical\"}])],\n",
    "        \"small\": [AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"high\"}])],\n",
    "        \"medium\": [AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"normal\"}])],\n",
    "        \"large\": [AddLiterals(data=[{\"alias\": \"priority\", \"value\": \"batch\"}])],\n",
    "    },\n",
    "    split_function=split_by_category,\n",
    "    name=\"Category Processing (Fixed)\",\n",
    "    repartition_output_to_original=True,  # Rebalance after append\n",
    ")\n",
    "\n",
    "# Now skew is fixed after the split\n",
    "# *** LogDataSkew *** Partitions: 8, Skew: 1.25x ‚úì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Worker Package Diagnostics\n",
    "\n",
    "When you don't manage the Spark cluster directly (Databricks, EMR, company clusters), package version mismatches between driver and workers cause cryptic errors.\n",
    "\n",
    "### 4.1 Check if a Package is Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lib-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas installed on workers: True\n",
      "xgboost installed on workers: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from nebula.spark_udfs import lib_in_spark_workers, lib_version_in_spark_workers\n",
    "\n",
    "# Check if pandas is available on workers\n",
    "has_pandas = lib_in_spark_workers(spark, \"pandas\")\n",
    "print(f\"pandas installed on workers: {has_pandas}\")\n",
    "\n",
    "# Check a package that might not be there\n",
    "has_xgboost = lib_in_spark_workers(spark, \"xgboost\")\n",
    "print(f\"xgboost installed on workers: {has_xgboost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "version-check",
   "metadata": {},
   "source": [
    "### 4.2 Check Package Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "version-check-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker pandas version: 2.3.3\n",
      "Driver pandas version: 2.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Get the version installed on workers\n",
    "pandas_version = lib_version_in_spark_workers(spark, \"pandas\")\n",
    "print(f\"Worker pandas version: {pandas_version}\")\n",
    "\n",
    "# Compare with driver\n",
    "import pandas as pd\n",
    "print(f\"Driver pandas version: {pd.__version__}\")\n",
    "\n",
    "# Mismatch? That might explain your UDF errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-pattern",
   "metadata": {},
   "source": [
    "### 4.3 Diagnostic Pattern\n",
    "\n",
    "When debugging cluster issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "diagnostic-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CLUSTER DIAGNOSTICS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì pandas: 2.3.3\n",
      "‚úì numpy: 2.2.6\n",
      "‚úì pyarrow: 22.0.0\n",
      "‚úì nebula: Unable to determine the package version\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def diagnose_cluster(spark, packages: list[str]):\n",
    "    \"\"\"Quick cluster diagnostic.\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"CLUSTER DIAGNOSTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for pkg in packages:\n",
    "        installed = lib_in_spark_workers(spark, pkg)\n",
    "        if installed:\n",
    "            version = lib_version_in_spark_workers(spark, pkg)\n",
    "            print(f\"‚úì {pkg}: {version}\")\n",
    "        else:\n",
    "            print(f\"‚úó {pkg}: NOT INSTALLED\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Usage\n",
    "diagnose_cluster(spark, [\"pandas\", \"numpy\", \"pyarrow\", \"nebula\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Tool | Purpose | When to Use |\n",
    "|------|---------|-------------|\n",
    "| `CpuInfo` | Report worker CPU types | Start of pipeline, debugging performance |\n",
    "| `LogDataSkew` | Monitor partition distribution | Interleaved or after joins/splits |\n",
    "| `lib_in_spark_workers` | Check package availability | Debugging UDF failures |\n",
    "| `lib_version_in_spark_workers` | Check package versions | Driver/worker version mismatch |\n",
    "\n",
    "**Skew Prevention:**\n",
    "- Use `repartition_output_to_original=True` on split pipelines\n",
    "- Monitor with `LogDataSkew` as interleaved transformer\n",
    "- Inject `force_interleaved_transformer=LogDataSkew()` at runtime for debugging\n",
    "\n",
    "**Cluster Debugging:**\n",
    "- When UDFs fail mysteriously, check package versions\n",
    "- Driver version ‚â† worker version is a common issue on managed clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
