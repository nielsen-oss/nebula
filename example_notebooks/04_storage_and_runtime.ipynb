{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 04 - Storage & Runtime Dynamics\n",
    "\n",
    "This notebook covers Nebula's runtime features for managing state, debugging, and handling failures:\n",
    "\n",
    "| Part | Topic | Description |\n",
    "|------|-------|-------------|\n",
    "| **1** | Nebula Storage | Store and retrieve objects during pipeline execution |\n",
    "| **2** | Pipeline Keywords | Declarative storage operations in pipelines |\n",
    "| **3** | LazyWrapper | Runtime parameter resolution from storage |\n",
    "| **4** | Interleaved Transformers | Inject debug transformers between steps |\n",
    "| **5** | Failure Recovery | Retrieve DataFrames when pipelines fail |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from nebula import TransformerPipeline\n",
    "from nebula.storage import nebula_storage as ns\n",
    "from nebula.base import Transformer, LazyWrapper\n",
    "from nebula.transformers import (\n",
    "    AddLiterals,\n",
    "    AssertNotEmpty,\n",
    "    DropColumns,\n",
    "    DropNulls,\n",
    "    Filter,\n",
    "    SelectColumns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sample-data",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>order_id</th><th>customer</th><th>amount</th><th>status</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;alice&quot;</td><td>150.0</td><td>&quot;completed&quot;</td></tr><tr><td>2</td><td>&quot;bob&quot;</td><td>75.0</td><td>&quot;completed&quot;</td></tr><tr><td>3</td><td>&quot;alice&quot;</td><td>200.0</td><td>&quot;pending&quot;</td></tr><tr><td>4</td><td>&quot;carol&quot;</td><td>50.0</td><td>&quot;completed&quot;</td></tr><tr><td>5</td><td>&quot;bob&quot;</td><td>300.0</td><td>&quot;pending&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────┬──────────┬────────┬───────────┐\n",
       "│ order_id ┆ customer ┆ amount ┆ status    │\n",
       "│ ---      ┆ ---      ┆ ---    ┆ ---       │\n",
       "│ i64      ┆ str      ┆ f64    ┆ str       │\n",
       "╞══════════╪══════════╪════════╪═══════════╡\n",
       "│ 1        ┆ alice    ┆ 150.0  ┆ completed │\n",
       "│ 2        ┆ bob      ┆ 75.0   ┆ completed │\n",
       "│ 3        ┆ alice    ┆ 200.0  ┆ pending   │\n",
       "│ 4        ┆ carol    ┆ 50.0   ┆ completed │\n",
       "│ 5        ┆ bob      ┆ 300.0  ┆ pending   │\n",
       "└──────────┴──────────┴────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample e-commerce data\n",
    "orders = pl.DataFrame({\n",
    "    \"order_id\": [1, 2, 3, 4, 5],\n",
    "    \"customer\": [\"alice\", \"bob\", \"alice\", \"carol\", \"bob\"],\n",
    "    \"amount\": [150.0, 75.0, 200.0, 50.0, 300.0],\n",
    "    \"status\": [\"completed\", \"completed\", \"pending\", \"completed\", \"pending\"],\n",
    "})\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Nebula Storage\n",
    "\n",
    "Nebula storage is an in-memory key-value store that lives within your Python process. It's useful for:\n",
    "\n",
    "- **Passing data between transformers** (e.g., a DataFrame for joining)\n",
    "- **Storing intermediate results** for debugging\n",
    "- **Recovering from failures** (automatic - see Part 5)\n",
    "\n",
    "### 1.1 Basic API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "storage-basics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:25,726 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:25,745 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:25,748 | [INFO]: Nebula Storage: setting an object (<class 'int'>) with the key \"my_number\". \n",
      "2025-12-26 01:09:25,750 | [INFO]: Nebula Storage: setting an object (<class 'list'>) with the key \"my_list\". \n",
      "2025-12-26 01:09:25,751 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"my_df\". \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 42\n",
      "List: [1, 2, 3]\n",
      "DataFrame rows: 5\n"
     ]
    }
   ],
   "source": [
    "# Always start fresh\n",
    "ns.clear()\n",
    "\n",
    "# Store any Python object\n",
    "ns.set(\"my_number\", 42)\n",
    "ns.set(\"my_list\", [1, 2, 3])\n",
    "ns.set(\"my_df\", orders)\n",
    "\n",
    "# Retrieve objects\n",
    "print(f\"Number: {ns.get('my_number')}\")\n",
    "print(f\"List: {ns.get('my_list')}\")\n",
    "print(f\"DataFrame rows: {len(ns.get('my_df'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "storage-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['my_df', 'my_list', 'my_number']\n",
      "Count: 3\n",
      "'my_number' exists: True\n",
      "'unknown' exists: False\n"
     ]
    }
   ],
   "source": [
    "# Inspect storage state\n",
    "print(f\"Keys: {ns.list_keys()}\")\n",
    "print(f\"Count: {ns.count_objects()}\")\n",
    "print(f\"'my_number' exists: {ns.isin('my_number')}\")\n",
    "print(f\"'unknown' exists: {ns.isin('unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "storage-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:27,257 | [INFO]: Nebula Storage: clear key \"my_number\". \n",
      "2025-12-26 01:09:27,258 | [INFO]: Nebula Storage: 2 keys remained after clearing. \n",
      "2025-12-26 01:09:27,259 | [INFO]: Nebula Storage: clear user-defined keys. \n",
      "2025-12-26 01:09:27,261 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:27,262 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"temp\". \n",
      "2025-12-26 01:09:27,262 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:27,263 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After clearing 'my_number': ['my_df', 'my_list']\n",
      "After clearing list: []\n",
      "After clear(): []\n"
     ]
    }
   ],
   "source": [
    "# Clear specific keys\n",
    "ns.clear(\"my_number\")\n",
    "print(f\"After clearing 'my_number': {ns.list_keys()}\")\n",
    "\n",
    "# Clear multiple keys\n",
    "ns.clear([\"my_list\", \"my_df\"])\n",
    "print(f\"After clearing list: {ns.list_keys()}\")\n",
    "\n",
    "# Clear everything\n",
    "ns.set(\"temp\", \"value\")\n",
    "ns.clear()  # No argument = clear all\n",
    "print(f\"After clear(): {ns.list_keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overwrite-header",
   "metadata": {},
   "source": [
    "### 1.2 Overwriting Control\n",
    "\n",
    "By default, storage allows overwriting. You can disable this to catch accidental key collisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "overwrite-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:28,898 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:28,900 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:28,903 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"key\". \n",
      "2025-12-26 01:09:28,905 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"key\". \n",
      "2025-12-26 01:09:28,907 | [INFO]: Nebula Storage: disallow overwriting. \n",
      "2025-12-26 01:09:28,907 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"key\". \n",
      "2025-12-26 01:09:28,908 | [INFO]: Nebula Storage: allow overwriting. \n",
      "2025-12-26 01:09:28,909 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:28,910 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: second\n",
      "KeyError: 'Nebula Storage: key \"key\" already exists and overwriting is disabled.'\n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "# Default: overwriting allowed\n",
    "ns.set(\"key\", \"first\")\n",
    "ns.set(\"key\", \"second\")  # Silently overwrites\n",
    "print(f\"Value: {ns.get('key')}\")\n",
    "\n",
    "# Disable overwriting\n",
    "ns.disallow_overwriting()\n",
    "try:\n",
    "    ns.set(\"key\", \"third\")  # Raises KeyError\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "\n",
    "# Re-enable for rest of notebook\n",
    "ns.allow_overwriting()\n",
    "ns.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debug-mode-header",
   "metadata": {},
   "source": [
    "### 1.3 Debug Mode\n",
    "\n",
    "Debug mode lets you store objects conditionally. When debug mode is **off**, `debug=True` stores are silently skipped. This lets you add extensive debugging without modifying code for production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debug-mode-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:30,293 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:30,296 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:30,298 | [INFO]: Nebula Storage: deactivate debug storage. \n",
      "2025-12-26 01:09:30,299 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"regular\". \n",
      "2025-12-26 01:09:30,299 | [INFO]: Nebula Storage: asked to set \"debug_data\" in debug mode but the storage debug is not active. The object will not be stored. \n",
      "2025-12-26 01:09:30,299 | [INFO]: Nebula Storage: activate debug storage. \n",
      "2025-12-26 01:09:30,299 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"debug_data\". \n",
      "2025-12-26 01:09:30,299 | [INFO]: Nebula Storage: deactivate debug storage. \n",
      "2025-12-26 01:09:30,299 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:30,299 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug OFF - keys: ['regular']\n",
      "Debug ON - keys: ['debug_data', 'regular']\n",
      "Debug mode active: True\n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "# Debug mode OFF (default)\n",
    "ns.allow_debug(False)\n",
    "ns.set(\"regular\", \"always stored\")\n",
    "ns.set(\"debug_data\", \"skipped when debug off\", debug=True)\n",
    "\n",
    "print(f\"Debug OFF - keys: {ns.list_keys()}\")\n",
    "\n",
    "# Debug mode ON\n",
    "ns.allow_debug(True)\n",
    "ns.set(\"debug_data\", \"now it's stored\", debug=True)\n",
    "\n",
    "print(f\"Debug ON - keys: {ns.list_keys()}\")\n",
    "print(f\"Debug mode active: {ns.is_debug_mode}\")\n",
    "\n",
    "ns.allow_debug(False)\n",
    "ns.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Pipeline Storage Keywords\n",
    "\n",
    "Instead of writing custom transformers to store data, use pipeline keywords - single-key dictionaries that integrate with the pipeline flow.\n",
    "\n",
    "| Keyword | Description |\n",
    "|---------|-------------|\n",
    "| `{\"store\": \"key\"}` | Store current DataFrame |\n",
    "| `{\"store_debug\": \"key\"}` | Store only if debug mode is active |\n",
    "| `{\"storage_debug_mode\": True/False}` | Toggle debug mode |\n",
    "| `{\"replace_with_stored_df\": \"key\"}` | Replace current DataFrame with stored one |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "keyword-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:31,870 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:31,872 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline *** (2 transformations)\n",
      " - Filter\n",
      "   --> Store df with key \"completed_orders\"\n",
      " - SelectColumns\n",
      "   --> Store df with key \"final_output\"\n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "pipe = TransformerPipeline([\n",
    "    Filter(input_col=\"status\", perform=\"keep\", operator=\"eq\", value=\"completed\"),\n",
    "    {\"store\": \"completed_orders\"},  # Store after filtering\n",
    "    SelectColumns(columns=[\"order_id\", \"amount\"]),\n",
    "    {\"store\": \"final_output\"},\n",
    "])\n",
    "\n",
    "pipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "keyword-store-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:32,612 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:32,613 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:09:32,619 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:09:32,619 | [INFO]:    --> Store df with key \"completed_orders\" \n",
      "2025-12-26 01:09:32,619 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"completed_orders\". \n",
      "2025-12-26 01:09:32,619 | [INFO]: Running 'SelectColumns' ... \n",
      "2025-12-26 01:09:32,619 | [INFO]: Completed 'SelectColumns' in 0.0s \n",
      "2025-12-26 01:09:32,631 | [INFO]:    --> Store df with key \"final_output\" \n",
      "2025-12-26 01:09:32,632 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"final_output\". \n",
      "2025-12-26 01:09:32,633 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stored keys: ['completed_orders', 'final_output']\n",
      "\n",
      "Completed orders (before select):\n",
      "shape: (3, 4)\n",
      "┌──────────┬──────────┬────────┬───────────┐\n",
      "│ order_id ┆ customer ┆ amount ┆ status    │\n",
      "│ ---      ┆ ---      ┆ ---    ┆ ---       │\n",
      "│ i64      ┆ str      ┆ f64    ┆ str       │\n",
      "╞══════════╪══════════╪════════╪═══════════╡\n",
      "│ 1        ┆ alice    ┆ 150.0  ┆ completed │\n",
      "│ 2        ┆ bob      ┆ 75.0   ┆ completed │\n",
      "│ 4        ┆ carol    ┆ 50.0   ┆ completed │\n",
      "└──────────┴──────────┴────────┴───────────┘\n",
      "\n",
      "Final output:\n",
      "shape: (3, 2)\n",
      "┌──────────┬────────┐\n",
      "│ order_id ┆ amount │\n",
      "│ ---      ┆ ---    │\n",
      "│ i64      ┆ f64    │\n",
      "╞══════════╪════════╡\n",
      "│ 1        ┆ 150.0  │\n",
      "│ 2        ┆ 75.0   │\n",
      "│ 4        ┆ 50.0   │\n",
      "└──────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(orders)\n",
    "\n",
    "print(f\"\\nStored keys: {ns.list_keys()}\")\n",
    "print(f\"\\nCompleted orders (before select):\")\n",
    "print(ns.get(\"completed_orders\"))\n",
    "print(f\"\\nFinal output:\")\n",
    "print(ns.get(\"final_output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keyword-debug-header",
   "metadata": {},
   "source": [
    "### 2.1 Debug Storage in Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "keyword-debug",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:34,098 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:34,099 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline *** (1 transformation)\n",
      "   --> Deactivate storage debug mode\n",
      " - Filter\n",
      "   --> Store df (debug) with key \"step1_skipped\"\n",
      "   --> Activate storage debug mode\n",
      "   --> Store df (debug) with key \"step2_stored\"\n",
      "   --> Deactivate storage debug mode\n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "pipe = TransformerPipeline([\n",
    "    {\"storage_debug_mode\": False},           # Start with debug OFF\n",
    "    Filter(input_col=\"amount\", perform=\"keep\", operator=\"gt\", value=100),\n",
    "    {\"store_debug\": \"step1_skipped\"},        # Skipped (debug off)\n",
    "    {\"storage_debug_mode\": True},            # Turn debug ON\n",
    "    {\"store_debug\": \"step2_stored\"},         # Stored (debug on)\n",
    "    {\"storage_debug_mode\": False},           # Turn debug OFF\n",
    "])\n",
    "\n",
    "pipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "keyword-debug-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:34,832 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:34,835 | [INFO]:    --> Deactivate storage debug mode \n",
      "2025-12-26 01:09:34,838 | [INFO]: Nebula Storage: deactivate debug storage. \n",
      "2025-12-26 01:09:34,839 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:09:34,844 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:09:34,844 | [INFO]:    --> Store df (debug) with key \"step1_skipped\" \n",
      "2025-12-26 01:09:34,844 | [INFO]: Nebula Storage: asked to set \"step1_skipped\" in debug mode but the storage debug is not active. The object will not be stored. \n",
      "2025-12-26 01:09:34,844 | [INFO]:    --> Activate storage debug mode \n",
      "2025-12-26 01:09:34,844 | [INFO]: Nebula Storage: activate debug storage. \n",
      "2025-12-26 01:09:34,844 | [INFO]:    --> Store df (debug) with key \"step2_stored\" \n",
      "2025-12-26 01:09:34,849 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"step2_stored\". \n",
      "2025-12-26 01:09:34,850 | [INFO]:    --> Deactivate storage debug mode \n",
      "2025-12-26 01:09:34,850 | [INFO]: Nebula Storage: deactivate debug storage. \n",
      "2025-12-26 01:09:34,850 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored keys: ['step2_stored']\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(orders)\n",
    "\n",
    "print(f\"Stored keys: {ns.list_keys()}\")\n",
    "# Note: 'step1_skipped' is NOT in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keyword-replace-header",
   "metadata": {},
   "source": [
    "### 2.2 Replacing the DataFrame Mid-Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "keyword-replace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:36,274 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:36,276 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:36,278 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"customer_tiers\". \n",
      "2025-12-26 01:09:36,280 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:36,280 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:09:36,284 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:09:36,284 | [INFO]:    --> Store df with key \"filtered_orders\" \n",
      "2025-12-26 01:09:36,284 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"filtered_orders\". \n",
      "2025-12-26 01:09:36,285 | [INFO]:    --> Load df from key \"customer_tiers\" \n",
      "2025-12-26 01:09:36,285 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:09:36,287 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:09:36,287 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (gold tier customers):\n",
      "shape: (1, 2)\n",
      "┌──────────┬──────┐\n",
      "│ customer ┆ tier │\n",
      "│ ---      ┆ ---  │\n",
      "│ str      ┆ str  │\n",
      "╞══════════╪══════╡\n",
      "│ alice    ┆ gold │\n",
      "└──────────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "# Pre-store a lookup table\n",
    "customer_tiers = pl.DataFrame({\n",
    "    \"customer\": [\"alice\", \"bob\", \"carol\"],\n",
    "    \"tier\": [\"gold\", \"silver\", \"bronze\"],\n",
    "})\n",
    "ns.set(\"customer_tiers\", customer_tiers)\n",
    "\n",
    "# Pipeline that swaps to a different DataFrame\n",
    "pipe = TransformerPipeline([\n",
    "    Filter(input_col=\"amount\", perform=\"keep\", operator=\"gt\", value=100),\n",
    "    {\"store\": \"filtered_orders\"},\n",
    "    {\"replace_with_stored_df\": \"customer_tiers\"},  # Switch to tiers table\n",
    "    Filter(input_col=\"tier\", perform=\"keep\", operator=\"eq\", value=\"gold\"),\n",
    "])\n",
    "\n",
    "result = pipe.run(orders)\n",
    "print(\"Result (gold tier customers):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: LazyWrapper - Runtime Parameter Resolution\n",
    "\n",
    "Sometimes transformer parameters aren't known until runtime - they depend on values computed by earlier pipeline steps. `LazyWrapper` defers transformer instantiation until `transform()` is called, resolving parameters from storage at that moment.\n",
    "\n",
    "### 3.1 Basic LazyWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lazy-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:37,874 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:37,876 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:37,878 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"discount_label\". \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline *** (1 transformation)\n",
      " - (Lazy) AddLiterals -> PARAMS: data=[{'alias': 'promo_code', 'value': 'ns.get(\"discount_label\")'}]\n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "# Simulate an earlier step storing a computed value\n",
    "ns.set(\"discount_label\", \"holiday_promo\")\n",
    "\n",
    "# LazyWrapper resolves (ns, \"key\") at transform time\n",
    "pipe = TransformerPipeline([\n",
    "    LazyWrapper(\n",
    "        AddLiterals,\n",
    "        data=[{\"alias\": \"promo_code\", \"value\": (ns, \"discount_label\")}]\n",
    "    ),\n",
    "])\n",
    "\n",
    "pipe.show(add_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lazy-basic-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:38,800 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:38,802 | [INFO]: Running '(Lazy) AddLiterals' ... \n",
      "2025-12-26 01:09:38,804 | [INFO]: Completed '(Lazy) AddLiterals' in 0.0s \n",
      "2025-12-26 01:09:38,807 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌──────────┬──────────┬────────┬───────────┬───────────────┐\n",
      "│ order_id ┆ customer ┆ amount ┆ status    ┆ promo_code    │\n",
      "│ ---      ┆ ---      ┆ ---    ┆ ---       ┆ ---           │\n",
      "│ i64      ┆ str      ┆ f64    ┆ str       ┆ str           │\n",
      "╞══════════╪══════════╪════════╪═══════════╪═══════════════╡\n",
      "│ 1        ┆ alice    ┆ 150.0  ┆ completed ┆ holiday_promo │\n",
      "│ 2        ┆ bob      ┆ 75.0   ┆ completed ┆ holiday_promo │\n",
      "│ 3        ┆ alice    ┆ 200.0  ┆ pending   ┆ holiday_promo │\n",
      "│ 4        ┆ carol    ┆ 50.0   ┆ completed ┆ holiday_promo │\n",
      "│ 5        ┆ bob      ┆ 300.0  ┆ pending   ┆ holiday_promo │\n",
      "└──────────┴──────────┴────────┴───────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(orders)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy-dynamic-header",
   "metadata": {},
   "source": [
    "### 3.2 Dynamic Parameters from Earlier Steps\n",
    "\n",
    "The real power: one transformer computes a value, stores it, and a later transformer uses it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lazy-dynamic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:40,439 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:40,440 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:40,443 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:40,443 | [INFO]: Running 'ComputeThreshold' ... \n",
      "2025-12-26 01:09:40,448 | [INFO]: Nebula Storage: setting an object (<class 'float'>) with the key \"computed_threshold\". \n",
      "2025-12-26 01:09:40,450 | [INFO]: Completed 'ComputeThreshold' in 0.0s \n",
      "2025-12-26 01:09:40,450 | [INFO]: Running '(Lazy) Filter' ... \n",
      "2025-12-26 01:09:40,453 | [INFO]: Completed '(Lazy) Filter' in 0.0s \n",
      "2025-12-26 01:09:40,455 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed threshold: 155.0\n",
      "\n",
      "Orders above average amount:\n",
      "shape: (2, 4)\n",
      "┌──────────┬──────────┬────────┬─────────┐\n",
      "│ order_id ┆ customer ┆ amount ┆ status  │\n",
      "│ ---      ┆ ---      ┆ ---    ┆ ---     │\n",
      "│ i64      ┆ str      ┆ f64    ┆ str     │\n",
      "╞══════════╪══════════╪════════╪═════════╡\n",
      "│ 3        ┆ alice    ┆ 200.0  ┆ pending │\n",
      "│ 5        ┆ bob      ┆ 300.0  ┆ pending │\n",
      "└──────────┴──────────┴────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "class ComputeThreshold(Transformer):\n",
    "    \"\"\"Compute and store a threshold based on data.\"\"\"\n",
    "    def _transform_nw(self, df):\n",
    "        # In reality, this might be a complex calculation\n",
    "        import narwhals as nw\n",
    "        avg = df.select(nw.col(\"amount\").mean()).to_native().item()\n",
    "        ns.set(\"computed_threshold\", avg)\n",
    "        print(f\"Computed threshold: {avg}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "pipe = TransformerPipeline([\n",
    "    ComputeThreshold(),\n",
    "    # Filter uses the threshold computed above\n",
    "    LazyWrapper(\n",
    "        Filter,\n",
    "        input_col=\"amount\",\n",
    "        perform=\"keep\",\n",
    "        operator=\"gt\",\n",
    "        value=(ns, \"computed_threshold\"),\n",
    "    ),\n",
    "])\n",
    "\n",
    "result = pipe.run(orders)\n",
    "print(f\"\\nOrders above average amount:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy-nested-header",
   "metadata": {},
   "source": [
    "### 3.3 Nested Lazy Parameters\n",
    "\n",
    "Lazy references work at any nesting depth within parameter structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lazy-nested",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:42,867 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:42,869 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:42,869 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"col1_value\". \n",
      "2025-12-26 01:09:42,871 | [INFO]: Nebula Storage: setting an object (<class 'int'>) with the key \"col2_value\". \n",
      "2025-12-26 01:09:42,873 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:42,874 | [INFO]: Running '(Lazy) AddLiterals' ... \n",
      "2025-12-26 01:09:42,876 | [INFO]: Completed '(Lazy) AddLiterals' in 0.0s \n",
      "2025-12-26 01:09:42,876 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 7)\n",
      "┌──────────┬──────────┬────────┬───────────┬────────────┬─────────────────────┬──────────────┐\n",
      "│ order_id ┆ customer ┆ amount ┆ status    ┆ static_col ┆ dynamic_col1        ┆ dynamic_col2 │\n",
      "│ ---      ┆ ---      ┆ ---    ┆ ---       ┆ ---        ┆ ---                 ┆ ---          │\n",
      "│ i64      ┆ str      ┆ f64    ┆ str       ┆ str        ┆ str                 ┆ i32          │\n",
      "╞══════════╪══════════╪════════╪═══════════╪════════════╪═════════════════════╪══════════════╡\n",
      "│ 1        ┆ alice    ┆ 150.0  ┆ completed ┆ hardcoded  ┆ computed_at_runtime ┆ 999          │\n",
      "│ 2        ┆ bob      ┆ 75.0   ┆ completed ┆ hardcoded  ┆ computed_at_runtime ┆ 999          │\n",
      "└──────────┴──────────┴────────┴───────────┴────────────┴─────────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "ns.set(\"col1_value\", \"computed_at_runtime\")\n",
    "ns.set(\"col2_value\", 999)\n",
    "\n",
    "# Lazy references nested inside list of dicts\n",
    "pipe = TransformerPipeline([\n",
    "    LazyWrapper(\n",
    "        AddLiterals,\n",
    "        data=[\n",
    "            {\"alias\": \"static_col\", \"value\": \"hardcoded\"},\n",
    "            {\"alias\": \"dynamic_col1\", \"value\": (ns, \"col1_value\")},\n",
    "            {\"alias\": \"dynamic_col2\", \"value\": (ns, \"col2_value\")},\n",
    "        ]\n",
    "    ),\n",
    "])\n",
    "\n",
    "result = pipe.run(orders.head(2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy-yaml-header",
   "metadata": {},
   "source": [
    "### 3.4 JSON/YAML Syntax: `__ns__` Prefix\n",
    "\n",
    "In JSON/YAML configs, use the `__ns__` prefix instead of `(ns, \"key\")` tuples:\n",
    "\n",
    "```yaml\n",
    "- transformer: AddLiterals\n",
    "  lazy: true\n",
    "  params:\n",
    "    data:\n",
    "      - alias: dynamic_col\n",
    "        value: \"__ns__my_storage_key\"\n",
    "```\n",
    "\n",
    "The `lazy: true` flag tells the loader to wrap the transformer in `LazyWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lazy-yaml-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:49,262 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:09:49,263 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:09:49,264 | [INFO]: Nebula Storage: setting an object (<class 'str'>) with the key \"runtime_value\". \n",
      "2025-12-26 01:09:49,267 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:49,267 | [INFO]: Running '(Lazy) AddLiterals' ... \n",
      "2025-12-26 01:09:49,272 | [INFO]: Completed '(Lazy) AddLiterals' in 0.0s \n",
      "2025-12-26 01:09:49,272 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 5)\n",
      "┌──────────┬──────────┬────────┬───────────┬─────────────────────┐\n",
      "│ order_id ┆ customer ┆ amount ┆ status    ┆ new_col             │\n",
      "│ ---      ┆ ---      ┆ ---    ┆ ---       ┆ ---                 │\n",
      "│ i64      ┆ str      ┆ f64    ┆ str       ┆ str                 │\n",
      "╞══════════╪══════════╪════════╪═══════════╪═════════════════════╡\n",
      "│ 1        ┆ alice    ┆ 150.0  ┆ completed ┆ loaded_from_storage │\n",
      "│ 2        ┆ bob      ┆ 75.0   ┆ completed ┆ loaded_from_storage │\n",
      "└──────────┴──────────┴────────┴───────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from nebula.pipelines.pipeline_loader import load_pipeline\n",
    "\n",
    "ns.clear()\n",
    "ns.set(\"runtime_value\", \"loaded_from_storage\")\n",
    "\n",
    "config = {\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "            \"transformer\": \"AddLiterals\",\n",
    "            \"lazy\": True,\n",
    "            \"params\": {\n",
    "                \"data\": [\n",
    "                    {\"alias\": \"new_col\", \"value\": \"__ns__runtime_value\"}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "pipe = load_pipeline(config)\n",
    "result = pipe.run(orders.head(2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Interleaved Transformers\n",
    "\n",
    "Interleaved transformers are injected between each step of your pipeline - useful for debugging, logging, or validation during development.\n",
    "\n",
    "### 4.1 The `interleaved` Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "interleaved-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline *** (5 transformations)\n",
      " - Filter\n",
      " - LogRowCount\n",
      " - Filter\n",
      " - LogRowCount\n",
      " - SelectColumns\n"
     ]
    }
   ],
   "source": [
    "class LogRowCount(Transformer):\n",
    "    \"\"\"Debug transformer that logs row count.\"\"\"\n",
    "    def _transform_nw(self, df):\n",
    "        print(f\"  [DEBUG] Row count: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "pipe = TransformerPipeline(\n",
    "    [\n",
    "        Filter(input_col=\"status\", perform=\"keep\", operator=\"eq\", value=\"completed\"),\n",
    "        Filter(input_col=\"amount\", perform=\"keep\", operator=\"gt\", value=50),\n",
    "        SelectColumns(columns=[\"order_id\", \"amount\"]),\n",
    "    ],\n",
    "    interleaved=[LogRowCount()],  # Injected after each step\n",
    ")\n",
    "\n",
    "pipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "interleaved-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:57,742 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:57,744 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:09:57,748 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:09:57,750 | [INFO]: Running 'LogRowCount' ... \n",
      "2025-12-26 01:09:57,753 | [INFO]: Completed 'LogRowCount' in 0.0s \n",
      "2025-12-26 01:09:57,753 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:09:57,753 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:09:57,753 | [INFO]: Running 'LogRowCount' ... \n",
      "2025-12-26 01:09:57,753 | [INFO]: Completed 'LogRowCount' in 0.0s \n",
      "2025-12-26 01:09:57,753 | [INFO]: Running 'SelectColumns' ... \n",
      "2025-12-26 01:09:57,753 | [INFO]: Completed 'SelectColumns' in 0.0s \n",
      "2025-12-26 01:09:57,753 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [DEBUG] Row count: 3\n",
      "  [DEBUG] Row count: 2\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interleaved-options-header",
   "metadata": {},
   "source": [
    "### 4.2 Prepend and Append Options\n",
    "\n",
    "Control whether interleaved transformers run at the start/end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interleaved-options",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:09:59,029 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:09:59,029 | [INFO]: Running 'LogRowCount' ... \n",
      "2025-12-26 01:09:59,029 | [INFO]: Completed 'LogRowCount' in 0.0s \n",
      "2025-12-26 01:09:59,029 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:09:59,029 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:09:59,029 | [INFO]: Running 'LogRowCount' ... \n",
      "2025-12-26 01:09:59,029 | [INFO]: Completed 'LogRowCount' in 0.0s \n",
      "2025-12-26 01:09:59,029 | [INFO]: Running 'SelectColumns' ... \n",
      "2025-12-26 01:09:59,029 | [INFO]: Completed 'SelectColumns' in 0.0s \n",
      "2025-12-26 01:09:59,029 | [INFO]: Running 'LogRowCount' ... \n",
      "2025-12-26 01:09:59,029 | [INFO]: Completed 'LogRowCount' in 0.0s \n",
      "2025-12-26 01:09:59,045 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With prepend and append:\n",
      "  [DEBUG] Row count: 5\n",
      "  [DEBUG] Row count: 3\n",
      "  [DEBUG] Row count: 3\n"
     ]
    }
   ],
   "source": [
    "pipe = TransformerPipeline(\n",
    "    [\n",
    "        Filter(input_col=\"amount\", perform=\"keep\", operator=\"gt\", value=100),\n",
    "        SelectColumns(columns=[\"order_id\", \"amount\"]),\n",
    "    ],\n",
    "    interleaved=[LogRowCount()],\n",
    "    prepend_interleaved=True,   # Also run BEFORE first transformer\n",
    "    append_interleaved=True,    # Also run AFTER last transformer\n",
    ")\n",
    "\n",
    "print(\"With prepend and append:\")\n",
    "result = pipe.run(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interleaved-runtime-header",
   "metadata": {},
   "source": [
    "### 4.3 Runtime Injection: `force_interleaved_transformer`\n",
    "\n",
    "Inject a transformer at runtime without modifying the pipeline definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "interleaved-runtime",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:10:00,554 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:10:00,557 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:10:00,562 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:10:00,563 | [INFO]: Running 'SelectColumns' ... \n",
      "2025-12-26 01:10:00,567 | [INFO]: Completed 'SelectColumns' in 0.0s \n",
      "2025-12-26 01:10:00,569 | [INFO]: Pipeline completed in 0.0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with injected debug transformer:\n",
      "  [DEBUG] Row count: 3\n",
      "  [DEBUG] Row count: 3\n"
     ]
    }
   ],
   "source": [
    "# Pipeline defined without interleaved\n",
    "pipe = TransformerPipeline([\n",
    "    Filter(input_col=\"status\", perform=\"keep\", operator=\"eq\", value=\"completed\"),\n",
    "    SelectColumns(columns=[\"order_id\", \"customer\", \"amount\"]),\n",
    "])\n",
    "\n",
    "# Inject at runtime for this specific run\n",
    "print(\"Running with injected debug transformer:\")\n",
    "result = pipe.run(orders, force_interleaved_transformer=LogRowCount())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Failure Recovery\n",
    "\n",
    "When a pipeline fails, Nebula automatically caches the DataFrame(s) that were about to be processed. This lets you inspect the data and debug without re-running expensive earlier steps.\n",
    "\n",
    "### 5.1 Transformer Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fail-transformer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:10:02,266 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:10:02,269 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "class BrokenTransformer(Transformer):\n",
    "    \"\"\"A transformer that always fails.\"\"\"\n",
    "    def _transform_nw(self, df):\n",
    "        raise ValueError(\"Something went wrong!\")\n",
    "\n",
    "\n",
    "pipe = TransformerPipeline([\n",
    "    Filter(input_col=\"amount\", perform=\"keep\", operator=\"gt\", value=100),\n",
    "    BrokenTransformer(),  # This will fail\n",
    "    SelectColumns(columns=[\"order_id\"]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fail-transformer-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:10:03,099 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:10:03,102 | [INFO]: Running 'Filter' ... \n",
      "2025-12-26 01:10:03,106 | [INFO]: Completed 'Filter' in 0.0s \n",
      "2025-12-26 01:10:03,106 | [INFO]: Running 'BrokenTransformer' ... \n",
      "2025-12-26 01:10:03,106 | [ERROR]: Error at node BrokenTransformer_2d10d9@2: Something went wrong! \n",
      "2025-12-26 01:10:03,106 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"FAIL_DF_transformer:BrokenTransformer\". \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline failed: Get the dataframe(s) before the failure in the nebula storage with the key(s): ['transformer:BrokenTransformer']\n",
      "Original Error:\n",
      "Something went wrong!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipe.run(orders)\n",
    "except ValueError as e:\n",
    "    print(f\"Pipeline failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fail-transformer-recover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached keys: ['FAIL_DF_transformer:BrokenTransformer']\n",
      "\n",
      "DataFrame before failure:\n",
      "shape: (3, 4)\n",
      "┌──────────┬──────────┬────────┬───────────┐\n",
      "│ order_id ┆ customer ┆ amount ┆ status    │\n",
      "│ ---      ┆ ---      ┆ ---    ┆ ---       │\n",
      "│ i64      ┆ str      ┆ f64    ┆ str       │\n",
      "╞══════════╪══════════╪════════╪═══════════╡\n",
      "│ 1        ┆ alice    ┆ 150.0  ┆ completed │\n",
      "│ 3        ┆ alice    ┆ 200.0  ┆ pending   │\n",
      "│ 5        ┆ bob      ┆ 300.0  ┆ pending   │\n",
      "└──────────┴──────────┴────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Check what was cached\n",
    "print(f\"Cached keys: {ns.list_keys()}\")\n",
    "\n",
    "# Retrieve the DataFrame that was about to be processed\n",
    "failed_df = ns.get(\"FAIL_DF_transformer:BrokenTransformer\")\n",
    "print(f\"\\nDataFrame before failure:\")\n",
    "print(failed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fail-split-header",
   "metadata": {},
   "source": [
    "### 5.2 Split Pipeline Merge Failure\n",
    "\n",
    "When split branches can't be merged (schema mismatch), Nebula caches all branch DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fail-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:10:04,966 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:10:04,969 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n"
     ]
    }
   ],
   "source": [
    "ns.clear()\n",
    "\n",
    "def split_by_amount(df):\n",
    "    \"\"\"Split into low and high value orders.\"\"\"\n",
    "    return {\n",
    "        \"low\": df.filter(pl.col(\"amount\") < 100),\n",
    "        \"high\": df.filter(pl.col(\"amount\") >= 100),\n",
    "    }\n",
    "\n",
    "\n",
    "# Each branch drops a DIFFERENT column - merge will fail!\n",
    "pipe = TransformerPipeline(\n",
    "    {\n",
    "        \"low\": [DropColumns(columns=\"customer\")],   # Drops 'customer'\n",
    "        \"high\": [DropColumns(columns=\"status\")],    # Drops 'status'\n",
    "    },\n",
    "    split_function=split_by_amount,\n",
    "    # allow_missing_columns=False (default) - will fail on merge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fail-split-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:10:05,549 | [INFO]: Starting pipeline \n",
      "2025-12-26 01:10:05,556 | [INFO]: Entering split \n",
      "2025-12-26 01:10:05,559 | [INFO]: Running 'DropColumns' ... \n",
      "2025-12-26 01:10:05,562 | [INFO]: Completed 'DropColumns' in 0.0s \n",
      "2025-12-26 01:10:05,562 | [INFO]: Running 'DropColumns' ... \n",
      "2025-12-26 01:10:05,566 | [INFO]: Completed 'DropColumns' in 0.0s \n",
      "2025-12-26 01:10:05,568 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"FAIL_DF_high-df-before-appending:append\". \n",
      "2025-12-26 01:10:05,569 | [INFO]: Nebula Storage: setting an object (<class 'polars.dataframe.frame.DataFrame'>) with the key \"FAIL_DF_low-df-before-appending:append\". \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline failed: ValueError\n",
      "Message: Get the dataframe(s) before the failure in the nebula storage with the key(s): ['high-df-before-appe...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipe.run(orders)\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline failed: {type(e).__name__}\")\n",
    "    print(f\"Message: {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fail-split-recover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached keys: ['FAIL_DF_high-df-before-appending:append', 'FAIL_DF_low-df-before-appending:append']\n",
      "\n",
      "FAIL_DF_high-df-before-appending:append:\n",
      "  Columns: ['order_id', 'customer', 'amount']\n",
      "  Rows: 3\n",
      "\n",
      "FAIL_DF_low-df-before-appending:append:\n",
      "  Columns: ['order_id', 'amount', 'status']\n",
      "  Rows: 2\n"
     ]
    }
   ],
   "source": [
    "# Check cached DataFrames\n",
    "print(f\"Cached keys: {ns.list_keys()}\")\n",
    "\n",
    "# Retrieve each branch's result to debug\n",
    "for key in ns.list_keys():\n",
    "    if key.startswith(\"FAIL_DF_\"):\n",
    "        df = ns.get(key)\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(f\"  Columns: {df.columns}\")\n",
    "        print(f\"  Rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fail-keys-header",
   "metadata": {},
   "source": [
    "### 5.3 Failure Cache Key Patterns\n",
    "\n",
    "The failure cache uses predictable key patterns:\n",
    "\n",
    "| Failure Type | Key Pattern |\n",
    "|--------------|-------------|\n",
    "| Transformer | `FAIL_DF_transformer:TransformerName` |\n",
    "| Function | `FAIL_DF_function:function_name` |\n",
    "| Split fork | `FAIL_DF_fork:split` |\n",
    "| Before append | `FAIL_DF_{branch}-df-before-appending:append` |\n",
    "| Before join | `FAIL_DF_join-left-df:join`, `FAIL_DF_join-right-df:join` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Feature | Use Case |\n",
    "|---------|----------|\n",
    "| **nebula_storage** | Share data between transformers, store intermediates |\n",
    "| **Debug mode** | Conditional storage for dev vs prod |\n",
    "| **Pipeline keywords** | Declarative storage in pipeline definition |\n",
    "| **LazyWrapper** | Runtime parameter resolution from storage |\n",
    "| **Interleaved** | Debug/logging transformers between steps |\n",
    "| **Failure cache** | Automatic DataFrame recovery on errors |\n",
    "\n",
    "These features are particularly useful during development and debugging in notebooks, where you can inspect stored DataFrames interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:10:07,576 | [INFO]: Nebula Storage: clear. \n",
      "2025-12-26 01:10:07,578 | [INFO]: Nebula Storage: 0 keys remained after clearing. \n",
      "2025-12-26 01:10:07,580 | [INFO]: Nebula Storage: deactivate debug storage. \n",
      "2025-12-26 01:10:07,583 | [INFO]: Nebula Storage: allow overwriting. \n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "ns.clear()\n",
    "ns.allow_debug(False)\n",
    "ns.allow_overwriting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebff5c-29a6-412b-bdcd-7e377d7ef455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
